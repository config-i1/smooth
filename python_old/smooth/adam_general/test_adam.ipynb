{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter magic for auto-reloading modules without restarting the notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.adam import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from core.checker import parameters_checker\n",
    "from typing import List, Union, Dict, Any\n",
    "from smooth.adam_general._adam_general import adam_fitter, adam_forecaster\n",
    "from core.utils.utils import measurement_inverter, scaler, calculate_likelihood, calculate_entropy, calculate_multistep_loss\n",
    "from numpy.linalg import eigvals\n",
    "import nlopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.estimator import estimator, selector\n",
    "from core.creator import creator, initialiser, architector, filler\n",
    "from core.utils.ic import ic_function\n",
    "\n",
    "from smooth.adam_general._adam_general import adam_fitter, adam_forecaster\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77714/4282208315.py:5: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  dates = pd.date_range(start='2023-01-01', periods=n_points, freq='M')  # Monthly frequency\n"
     ]
    }
   ],
   "source": [
    "# Generate random monthly time series data\n",
    "np.random.seed(41)  # For reproducibility\n",
    "n_points = 24  # 2 years of monthly data\n",
    "time_series = np.random.randint(1, 100, size=n_points).cumsum()  # Random walk with strictly positive integers\n",
    "dates = pd.date_range(start='2023-01-01', periods=n_points, freq='M')  # Monthly frequency\n",
    "ts_df = pd.DataFrame({'value': time_series}, index=dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multisteps = False,\n",
    "lb = None,\n",
    "ub = None,\n",
    "maxtime = None,\n",
    "print_level = 1, # 1 or 0\n",
    "maxeval = None,\n",
    "h = 12\n",
    "\n",
    "\n",
    "\n",
    "# Assume that the model is not provided\n",
    "# these will be default arguments\n",
    "profiles_recent_provided = False\n",
    "profiles_recent_table = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77714/1138340368.py:4: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  dates = pd.date_range(start='2023-01-01', periods=n_points, freq='M')  # Monthly frequency\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(65)  # For reproducibility\n",
    "n_points = 39  # 2 years of monthly data\n",
    "time_series = np.random.randint(1, 100, size=n_points).cumsum()  # Random walk with strictly positive integers\n",
    "dates = pd.date_range(start='2023-01-01', periods=n_points, freq='M')  # Monthly frequency\n",
    "ts_df = pd.DataFrame({'value': time_series}, index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.3449 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>lower_0.025</th>\n",
       "      <th>upper_0.975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2026-05-31</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1684.915675</td>\n",
       "      <td>1907.084325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-06-30</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1638.903040</td>\n",
       "      <td>1953.096960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-07-31</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1603.596305</td>\n",
       "      <td>1988.403695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-08-31</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1573.831349</td>\n",
       "      <td>2018.168651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-09-30</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1547.607897</td>\n",
       "      <td>2044.392103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-10-31</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1523.900084</td>\n",
       "      <td>2068.099916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-11-30</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1502.098500</td>\n",
       "      <td>2089.901500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-12-31</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1481.806081</td>\n",
       "      <td>2110.193919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027-01-31</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1462.747024</td>\n",
       "      <td>2129.252976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027-02-28</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1444.720519</td>\n",
       "      <td>2147.279481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027-03-31</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1427.574973</td>\n",
       "      <td>2164.425027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027-04-30</th>\n",
       "      <td>1796.0</td>\n",
       "      <td>1411.192609</td>\n",
       "      <td>2180.807391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mean  lower_0.025  upper_0.975\n",
       "2026-05-31  1796.0  1684.915675  1907.084325\n",
       "2026-06-30  1796.0  1638.903040  1953.096960\n",
       "2026-07-31  1796.0  1603.596305  1988.403695\n",
       "2026-08-31  1796.0  1573.831349  2018.168651\n",
       "2026-09-30  1796.0  1547.607897  2044.392103\n",
       "2026-10-31  1796.0  1523.900084  2068.099916\n",
       "2026-11-30  1796.0  1502.098500  2089.901500\n",
       "2026-12-31  1796.0  1481.806081  2110.193919\n",
       "2027-01-31  1796.0  1462.747024  2129.252976\n",
       "2027-02-28  1796.0  1444.720519  2147.279481\n",
       "2027-03-31  1796.0  1427.574973  2164.425027\n",
       "2027-04-30  1796.0  1411.192609  2180.807391"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = \"ANN\"\n",
    "lags = [12]\n",
    "\n",
    "np.random.seed(33)  # For reproducibility\n",
    "n_points = 39  # 2 years of monthly data\n",
    "time_series = np.random.randint(1, 100, size=n_points).cumsum()  # Random walk with strictly positive integers\n",
    "dates = pd.date_range(start='2023-01-01', periods=n_points, freq='ME')  # Monthly frequency\n",
    "ts_df = pd.DataFrame({'value': time_series}, index=dates)\n",
    "\n",
    "adam = Adam(model, lags)\n",
    "adam.fit(ts_df, h = h)\n",
    "#print(adam.adam_created)\n",
    "fc = adam.predict(level = 0.95)\n",
    "#print(adam.adam_created)\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AirPassengers dataset loaded:\n",
      "            value\n",
      "1949-01-01    112\n",
      "1949-02-01    118\n",
      "1949-03-01    132\n",
      "1949-04-01    129\n",
      "1949-05-01    121\n"
     ]
    }
   ],
   "source": [
    "# Load the AirPassengers dataset from Python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Create the AirPassengers dataset manually\n",
    "data = [\n",
    "    112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118,\n",
    "    115, 126, 141, 135, 125, 149, 170, 170, 158, 133, 114, 140,\n",
    "    145, 150, 178, 163, 172, 178, 199, 199, 184, 162, 146, 166,\n",
    "    171, 180, 193, 181, 183, 218, 230, 242, 209, 191, 172, 194,\n",
    "    196, 196, 236, 235, 229, 243, 264, 272, 237, 211, 180, 201,\n",
    "    204, 188, 235, 227, 234, 264, 302, 293, 259, 229, 203, 229,\n",
    "    242, 233, 267, 269, 270, 315, 364, 347, 312, 274, 237, 278,\n",
    "    284, 277, 317, 313, 318, 374, 413, 405, 355, 306, 271, 306,\n",
    "    315, 301, 356, 348, 355, 422, 465, 467, 404, 347, 305, 336,\n",
    "    340, 318, 362, 348, 363, 435, 491, 505, 404, 359, 310, 337,\n",
    "    360, 342, 406, 396, 420, 472, 548, 559, 463, 407, 362, 405,\n",
    "    417, 391, 419, 461, 472, 535, 622, 606, 508, 461, 390, 432\n",
    "]\n",
    "\n",
    "# Create a proper datetime index\n",
    "dates = pd.date_range(start='1949-01-01', periods=len(data), freq='MS')\n",
    "\n",
    "# Create a pandas Series with the data\n",
    "air_passengers_series = pd.Series(data, index=dates, name='AirPassengers')\n",
    "\n",
    "# Create a DataFrame with the time series\n",
    "ts_df = pd.DataFrame({'value': air_passengers_series})\n",
    "\n",
    "print(\"AirPassengers dataset loaded:\")\n",
    "print(ts_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>lower_0.025</th>\n",
       "      <th>upper_0.975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>446.745880</td>\n",
       "      <td>412.661507</td>\n",
       "      <td>482.163463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-02-01</th>\n",
       "      <td>421.460877</td>\n",
       "      <td>387.180603</td>\n",
       "      <td>457.174452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-03-01</th>\n",
       "      <td>475.544201</td>\n",
       "      <td>434.615693</td>\n",
       "      <td>518.288047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-04-01</th>\n",
       "      <td>499.710761</td>\n",
       "      <td>454.469776</td>\n",
       "      <td>547.067557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-05-01</th>\n",
       "      <td>512.928279</td>\n",
       "      <td>464.314130</td>\n",
       "      <td>563.927971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-06-01</th>\n",
       "      <td>585.301208</td>\n",
       "      <td>527.458097</td>\n",
       "      <td>646.110430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-07-01</th>\n",
       "      <td>675.128957</td>\n",
       "      <td>605.790786</td>\n",
       "      <td>748.169926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-08-01</th>\n",
       "      <td>666.561737</td>\n",
       "      <td>595.620269</td>\n",
       "      <td>741.436966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-09-01</th>\n",
       "      <td>553.760531</td>\n",
       "      <td>492.836822</td>\n",
       "      <td>618.183254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-10-01</th>\n",
       "      <td>491.464745</td>\n",
       "      <td>435.691134</td>\n",
       "      <td>550.548758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-11-01</th>\n",
       "      <td>419.377938</td>\n",
       "      <td>370.378185</td>\n",
       "      <td>471.377508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-12-01</th>\n",
       "      <td>463.966043</td>\n",
       "      <td>408.247531</td>\n",
       "      <td>523.196928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean  lower_0.025  upper_0.975\n",
       "1961-01-01  446.745880   412.661507   482.163463\n",
       "1961-02-01  421.460877   387.180603   457.174452\n",
       "1961-03-01  475.544201   434.615693   518.288047\n",
       "1961-04-01  499.710761   454.469776   547.067557\n",
       "1961-05-01  512.928279   464.314130   563.927971\n",
       "1961-06-01  585.301208   527.458097   646.110430\n",
       "1961-07-01  675.128957   605.790786   748.169926\n",
       "1961-08-01  666.561737   595.620269   741.436966\n",
       "1961-09-01  553.760531   492.836822   618.183254\n",
       "1961-10-01  491.464745   435.691134   550.548758\n",
       "1961-11-01  419.377938   370.378185   471.377508\n",
       "1961-12-01  463.966043   408.247531   523.196928"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"MAM\"\n",
    "lags = [12]\n",
    "adam = Adam(model, lags)\n",
    "adam.fit(ts_df, h = h)\n",
    "adam.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation progress:   11172228333944505661677278838994100%... Done!\n",
      "Execution time: 25.9201 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>lower_0.025</th>\n",
       "      <th>upper_0.975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>446.745880</td>\n",
       "      <td>412.661507</td>\n",
       "      <td>482.163463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-02-01</th>\n",
       "      <td>421.460877</td>\n",
       "      <td>387.180603</td>\n",
       "      <td>457.174452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-03-01</th>\n",
       "      <td>475.544201</td>\n",
       "      <td>434.615693</td>\n",
       "      <td>518.288047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-04-01</th>\n",
       "      <td>499.710761</td>\n",
       "      <td>454.469776</td>\n",
       "      <td>547.067557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-05-01</th>\n",
       "      <td>512.928279</td>\n",
       "      <td>464.314130</td>\n",
       "      <td>563.927971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-06-01</th>\n",
       "      <td>585.301208</td>\n",
       "      <td>527.458097</td>\n",
       "      <td>646.110430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-07-01</th>\n",
       "      <td>675.128957</td>\n",
       "      <td>605.790786</td>\n",
       "      <td>748.169926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-08-01</th>\n",
       "      <td>666.561737</td>\n",
       "      <td>595.620269</td>\n",
       "      <td>741.436966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-09-01</th>\n",
       "      <td>553.760531</td>\n",
       "      <td>492.836822</td>\n",
       "      <td>618.183254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-10-01</th>\n",
       "      <td>491.464745</td>\n",
       "      <td>435.691134</td>\n",
       "      <td>550.548758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-11-01</th>\n",
       "      <td>419.377938</td>\n",
       "      <td>370.378185</td>\n",
       "      <td>471.377508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-12-01</th>\n",
       "      <td>463.966043</td>\n",
       "      <td>408.247531</td>\n",
       "      <td>523.196928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean  lower_0.025  upper_0.975\n",
       "1961-01-01  446.745880   412.661507   482.163463\n",
       "1961-02-01  421.460877   387.180603   457.174452\n",
       "1961-03-01  475.544201   434.615693   518.288047\n",
       "1961-04-01  499.710761   454.469776   547.067557\n",
       "1961-05-01  512.928279   464.314130   563.927971\n",
       "1961-06-01  585.301208   527.458097   646.110430\n",
       "1961-07-01  675.128957   605.790786   748.169926\n",
       "1961-08-01  666.561737   595.620269   741.436966\n",
       "1961-09-01  553.760531   492.836822   618.183254\n",
       "1961-10-01  491.464745   435.691134   550.548758\n",
       "1961-11-01  419.377938   370.378185   471.377508\n",
       "1961-12-01  463.966043   408.247531   523.196928"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = \"ZXZ\"\n",
    "lags = [12]\n",
    "\n",
    "\n",
    "adam = Adam(model, lags, silent = False)\n",
    "adam.fit(ts_df, h = h)\n",
    "fc = adam.predict()\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parameters_number': [[16, 1, np.int64(17)], [4, 4]],\n",
       " 'n_states': 3,\n",
       " 'n_params': 4}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.params_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.039693409487978386)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigma(observations_dict, params_info, general, prepared_model):\n",
    "    vals = observations_dict['obs_in_sample'] - params_info['parameters_number'][0][-1]\n",
    "    # If the sample is too small, then use biased estimator\n",
    "    if vals <= 0:\n",
    "        vals = observations_dict['obs_in_sample']\n",
    "\n",
    "    residuals = prepared_model['residuals']\n",
    "    non_nan_mask = ~residuals.isna()\n",
    "\n",
    "    # Calculate sigma based on distribution type\n",
    "    if general['distribution'] in ['dnorm', 'dlaplace', 'ds', 'dgnorm', 'dt', 'dlogis', 'dalaplace']:\n",
    "        sigma = (residuals[non_nan_mask]**2).sum()\n",
    "    elif general['distribution'] in ['dlnorm', 'dllaplace', 'dls']:\n",
    "        sigma = (np.log(residuals[non_nan_mask])**2).sum()\n",
    "    elif general['distribution'] == 'dlgnorm':\n",
    "        sigma = (np.log(residuals[non_nan_mask] - extract_scale()**2/2)**2).sum()\n",
    "    elif general['distribution'] in ['dinvgauss', 'dgamma']:\n",
    "        # sigma = ((residuals[non_nan_mask] - 1)**2).sum()\n",
    "\n",
    "        # Important note: I have droped to -1 here to match the R.\n",
    "        # In case we have other distribution we might need to sum + 1 to make the match\n",
    "        # I cant find the source of the discrepancy.\n",
    "        sigma = ((residuals[non_nan_mask])**2).sum()\n",
    "\n",
    "    return np.sqrt(sigma/vals)\n",
    "\n",
    "#sigma(adam.observations_dict, adam.params_info, adam.general, adam.prepared_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covar_anal(lags_model, h, measurement, transition, persistence, s2):\n",
    "    \"\"\"\n",
    "    Returns analytical conditional h-steps ahead covariance matrix. Corrected Python version.\n",
    "    This is used in covar() method and in the construction of parametric prediction intervals.\n",
    "\n",
    "    Parameters:\n",
    "    - lags_model: list or array, model lags assigned to each state (e.g., [1, 1, 12])\n",
    "    - h: int, forecast horizon\n",
    "    - measurement: array, measurement matrix (typically h x n_components from forecaster).\n",
    "                   The function logic uses the first row measurement[0, :].\n",
    "    - transition: array, transition matrix (n_components x n_components)\n",
    "    - persistence: array, persistence vector (k_states,)\n",
    "    - s2: float, one-step-ahead variance\n",
    "\n",
    "    Returns:\n",
    "    - covar_mat: array, covariance matrix (h x h)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays and persistence is 1D\n",
    "    lags_model = np.array(lags_model)\n",
    "    measurement_matrix = np.array(measurement) # Keep original name for clarity\n",
    "    transition = np.array(transition)\n",
    "    persistence = np.array(persistence).flatten() # Ensure persistence is 1D\n",
    "\n",
    "    n_components = transition.shape[0]\n",
    "    k_states = len(persistence)\n",
    "    # print(len(measurement_matrix)) # Optional debug print\n",
    "\n",
    "    # --- Basic Input Validation ---\n",
    "    if n_components != transition.shape[1]:\n",
    "        raise ValueError(\"Transition matrix must be square.\")\n",
    "    # Validate measurement matrix dimensions more robustly\n",
    "    if measurement_matrix.ndim != 2 or measurement_matrix.shape[1] != n_components:\n",
    "         raise ValueError(f\"Measurement matrix shape {measurement_matrix.shape} incompatible with n_components {n_components}. Expecting (>=1, {n_components}).\")\n",
    "    if lags_model.shape[0] != k_states:\n",
    "         raise ValueError(f\"lags_model length {lags_model.shape[0]} must match persistence length {k_states}.\")\n",
    "    if k_states > n_components: # Allow k_states <= n_components (e.g. ARIMA components)\n",
    "        raise ValueError(f\"Number of states ({k_states}) from persistence vector cannot exceed transition matrix dimension ({n_components}).\")\n",
    "    if measurement_matrix.shape[0] < 1:\n",
    "        raise ValueError(\"Measurement matrix must have at least one row.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    # Use the first row of the measurement matrix, similar to R logic assumption\n",
    "    measurement_vector = measurement_matrix[0, :] # Shape (n_components,)\n",
    "\n",
    "    covar_mat = np.eye(h)\n",
    "    min_lag = np.min(lags_model) if len(lags_model) > 0 else h + 1 # Handle empty lags\n",
    "\n",
    "    if h > min_lag:\n",
    "        lags_unique = np.unique(lags_model)\n",
    "        steps = np.sort(lags_unique[lags_unique <= h])\n",
    "        steps_number = len(steps)\n",
    "        if steps_number == 0: # No relevant lags within horizon\n",
    "             # Multiply the matrix by the one-step-ahead variance\n",
    "             covar_mat = covar_mat * s2\n",
    "             return covar_mat\n",
    "\n",
    "        array_transition = np.zeros((n_components, n_components, steps_number))\n",
    "        # This array stores slices of the measurement_vector based on lags\n",
    "        array_measurement = np.zeros((1, n_components, steps_number))\n",
    "\n",
    "        for i in range(steps_number):\n",
    "            mask = (lags_model == steps[i]) # k_states long boolean mask\n",
    "            # Need to map k_states mask to n_components columns\n",
    "            component_mask = np.zeros(n_components, dtype=bool)\n",
    "            if k_states == n_components:\n",
    "                 component_mask = mask\n",
    "            else:\n",
    "                 # Assuming persistence corresponds to the first k_states components\n",
    "                 # This might need adjustment based on specific model structure if k_states < n_components\n",
    "                 component_mask[:k_states] = mask\n",
    "\n",
    "            if np.sum(component_mask) > 0:\n",
    "                 array_transition[:, component_mask, i] = transition[:, component_mask]\n",
    "                 # Assign parts of the single measurement_vector\n",
    "                 array_measurement[0, component_mask, i] = measurement_vector[component_mask]\n",
    "\n",
    "        # Holds values corresponding to R's cValues[i+1]\n",
    "        c_values = np.zeros(h)\n",
    "\n",
    "        # Prepare transition array\n",
    "        transition_powered = np.zeros((n_components, n_components, h, steps_number))\n",
    "        # Initialize first min(steps) time steps (Python index 0 to min(steps)-1)\n",
    "        current_min_step = min(steps) if len(steps) > 0 else 0\n",
    "        # Corrected loop: Iterate through time steps AND step numbers for initialization\n",
    "        for i in range(current_min_step):\n",
    "            for k in range(steps_number): # Add loop over the 4th dimension (steps_number)\n",
    "                transition_powered[:, :, i, k] = np.eye(n_components) # Assign to specific [:,:,i,k] slice\n",
    "\n",
    "        # Generate values for the transition matrix\n",
    "        # R loops i from (min(steps)+1) to h. Python loops i from min(steps) to h-1.\n",
    "        for i in range(current_min_step, h):\n",
    "            # R loops k from 1 to sum(steps<i). Python loops k from 0 to sum(steps < i+1)-1\n",
    "            num_inner_loops_k = np.sum(steps < (i + 1))\n",
    "            for k in range(num_inner_loops_k):\n",
    "                # This needs to be produced only for the lower lag (k=0).\n",
    "                # Then it will be reused for the higher ones.\n",
    "                if k == 0:  # R's k==1\n",
    "                    # R loops j from 1 to sum(steps<i). Python loops j from 0 to sum(steps < i+1)-1\n",
    "                    num_inner_loops_j = np.sum(steps < (i + 1)) # Same limit as k loop\n",
    "                    for j in range(num_inner_loops_j):\n",
    "                        # Condition uses R's i, which is Python's i + 1\n",
    "                        if steps[j] == 0: continue # Avoid division by zero\n",
    "                        if ((i + 1 - steps[k]) / steps[j] > 1): # Use Py i+1\n",
    "                            transition_new = array_transition[:, :, j]\n",
    "                        else:\n",
    "                            transition_new = np.eye(n_components)\n",
    "\n",
    "                        # Indexing transition_powered uses Python's i, k, j\n",
    "                        past_index = i - steps[j]\n",
    "                        if past_index < 0: # Ensure index is valid\n",
    "                            # This case might indicate an issue or need specific handling.\n",
    "                            # For now, assume identity if index is invalid.\n",
    "                             past_transition_powered = np.eye(n_components)\n",
    "                        else:\n",
    "                             past_transition_powered = transition_powered[:, :, past_index, k]\n",
    "\n",
    "\n",
    "                        # If this is a zero matrix, do simple multiplication\n",
    "                        if np.all(transition_powered[:, :, i, k] == 0):\n",
    "                            transition_powered[:, :, i, k] = (transition_new @ past_transition_powered)\n",
    "                        else:\n",
    "                            # Check that the multiplication is not an identity matrix\n",
    "                            new_term = transition_new @ past_transition_powered\n",
    "                            if not np.allclose(new_term, np.eye(n_components)): # Use allclose for float comparison\n",
    "                                transition_powered[:, :, i, k] = transition_powered[:, :, i, k] + new_term\n",
    "                else: # k > 0 (R's k > 1)\n",
    "                    # Copy the structure from the lower lags (k=0)\n",
    "                    # R: transitionPowered[,,i-steps[k]+1,1]; (Index 1 for k=1)\n",
    "                    # Py: transition_powered[:, :, i-steps[k]+1, 0] (Index 0 for k=0)\n",
    "                    time_index_copy = i - steps[k] + 1\n",
    "                    if time_index_copy < 0 or time_index_copy >= h : # Ensure index is valid within h dimension\n",
    "                         # Handle invalid index - maybe copy identity or latest available?\n",
    "                         # Copying identity might be safest default if state is unknown.\n",
    "                         transition_powered[:, :, i, k] = np.eye(n_components)\n",
    "                    else:\n",
    "                         transition_powered[:, :, i, k] = transition_powered[:, :, time_index_copy, 0]\n",
    "\n",
    "                # Generate values of cj\n",
    "                # Uses Python's i, k. Stores result in c_values[i] (maps to R's cValues[i+1])\n",
    "                try:\n",
    "                     # Ensure array_measurement slice has correct shape (1, n_components) before matmul\n",
    "                     meas_slice = array_measurement[:, :, k]\n",
    "                     if meas_slice.shape != (1, n_components):\n",
    "                         # This case shouldn't happen with current logic, but good practice\n",
    "                         raise ValueError(f\"Unexpected shape for array_measurement slice: {meas_slice.shape}\")\n",
    "\n",
    "                     c_values[i] = c_values[i] + meas_slice @ transition_powered[:, :, i, k] @ persistence\n",
    "                except Exception as e:\n",
    "                     print(f\"Error calculating c_values at i={i}, k={k}: {e}\")\n",
    "                     # Handle error, maybe set to NaN or break?\n",
    "                     c_values[i] = np.nan # Example: set to NaN\n",
    "                     # Optional: break # Exit inner loop k if error occurs\n",
    "\n",
    "\n",
    "        # Fill in diagonals\n",
    "        # R loops i from 2 to h. Uses cValues[i].\n",
    "        # Python loops i from 1 to h-1. Needs value corresponding to R's cValues[i+1], which is Py's c_values[i].\n",
    "        # <<< FIX START >>>\n",
    "        for i in range(1, h):\n",
    "            # Index i is valid for c_values (0 to h-1)\n",
    "            # Ensure c_values[i] is not NaN before squaring\n",
    "            if not np.isnan(c_values[i]):\n",
    "                c_val_sq = c_values[i]**2  # Use c_values[i] corresponding to R's cValues[i+1]\n",
    "                # Ensure covar_mat[i-1, i-1] is not NaN before adding\n",
    "                if not np.isnan(covar_mat[i-1, i-1]):\n",
    "                     covar_mat[i, i] = covar_mat[i-1, i-1] + c_val_sq\n",
    "                else:\n",
    "                     covar_mat[i, i] = np.nan # Propagate NaN\n",
    "            else:\n",
    "                covar_mat[i, i] = np.nan # Propagate NaN\n",
    "        # <<< FIX END >>>\n",
    "\n",
    "        # Fill in off-diagonals\n",
    "        # R loops i, j from 1 to h. Python loops i, j from 0 to h-1.\n",
    "        for i in range(h):\n",
    "            for j in range(h):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                elif i == 0:  # R's i==1\n",
    "                    # R uses cValues[j]. Python needs element corresponding to R's cValues[j+1], which is Py's c_values[j].\n",
    "                    if j >= 0 and j < len(c_values): # Check index validity for c_values[j]\n",
    "                         covar_mat[i, j] = c_values[j] # Use c_values[j] instead of c_values[j-1]\n",
    "                    else:\n",
    "                         # Handle cases where index j might be out of bounds for c_values (shouldn't happen if j<h)\n",
    "                         # Add check for NaN propagation\n",
    "                         if j >= 0 and j < len(c_values) and np.isnan(c_values[j]):\n",
    "                              covar_mat[i,j] = np.nan\n",
    "                         elif j==0: # Explicitly handle Py j=0 case if needed, maybe should be 0? R's cValues[1] is 0.\n",
    "                              covar_mat[i,j] = 0.0 # Tentatively set to 0.0 based on R cValues[1] initial value\n",
    "                         # If j is out of bounds, something else is wrong. Let it raise IndexError or handle as NaN?\n",
    "                elif i > j:\n",
    "                    covar_mat[i, j] = covar_mat[j, i] # Symmetry\n",
    "                else: # i < j\n",
    "                    # R: covarMat[i-1,j-1] + covarMat[1,j] * covarMat[1,i];\n",
    "                    # Py: covar_mat[i-1, j-1] + covar_mat[0, j] * covar_mat[0, i]\n",
    "                    # This recursive relation should now use the correctly filled first row/col\n",
    "                    if (i-1) >= 0 and (j-1) >= 0: # Check indices\n",
    "                        term1 = covar_mat[i-1, j-1]\n",
    "                        term2 = covar_mat[0, j]\n",
    "                        term3 = covar_mat[0, i]\n",
    "                        # Check for NaN before calculation\n",
    "                        if not (np.isnan(term1) or np.isnan(term2) or np.isnan(term3)):\n",
    "                            covar_mat[i, j] = term1 + term2 * term3\n",
    "                        else:\n",
    "                            covar_mat[i, j] = np.nan # Propagate NaN if components are NaN\n",
    "                    else:\n",
    "                        # Handle cases where indices i-1 or j-1 are invalid (shouldn't happen if i < j and i >= 1)\n",
    "                         covar_mat[i, j] = np.nan # Or some other default?\n",
    "\n",
    "    # Multiply the matrix by the one-step-ahead variance\n",
    "    covar_mat = covar_mat * s2\n",
    "\n",
    "    # Replace NaNs that might have occurred due to index issues or errors, if desired\n",
    "    # covar_mat = np.nan_to_num(covar_mat, nan=0.0) # Optional: replace NaN with 0\n",
    "\n",
    "    return covar_mat\n",
    "\n",
    "\n",
    "def var_anal(lags_model, h, measurement, transition, persistence, s2):\n",
    "    \"\"\"\n",
    "    Returns variances for the multiplicative error ETS models. Corrected Python version.\n",
    "\n",
    "    Parameters:\n",
    "    - lags_model: list or array, model lags assigned to each state (e.g., [1, 1, 12])\n",
    "    - h: int, forecast horizon\n",
    "    - measurement: array, measurement vector (Should be 1D, shape (n_components,))\n",
    "    - transition: array, transition matrix (n_components x n_components)\n",
    "    - persistence: array, persistence vector (k_states,)\n",
    "    - s2: float, one-step-ahead variance\n",
    "\n",
    "    Returns:\n",
    "    - var_mat: array, variance vector (h,)\n",
    "    \"\"\"\n",
    "    # Ensure inputs are numpy arrays and persistence is 1D\n",
    "    lags_model = np.array(lags_model)\n",
    "    measurement = np.array(measurement)\n",
    "    transition = np.array(transition)\n",
    "    persistence = np.array(persistence).flatten() # Ensure persistence is 1D\n",
    "\n",
    "    # Prepare the necessary parameters\n",
    "    lags_unique = np.unique(lags_model) # All unique lags present in the model definition\n",
    "    steps = np.sort(lags_unique[lags_unique <= h]) # Unique lags <= horizon h used for array_persistence_q\n",
    "    steps_number = len(steps)\n",
    "    n_components = transition.shape[0] # Number of rows/cols in transition matrix\n",
    "    k_states = len(persistence) # Number of state components\n",
    "\n",
    "    # --- Input dimension validation (optional but recommended) ---\n",
    "    if n_components != transition.shape[1]:\n",
    "        raise ValueError(\"Transition matrix must be square.\")\n",
    "    # Ensure measurement is treated as 1D for validation\n",
    "    if measurement.ndim > 1:\n",
    "         # Attempt to flatten if it makes sense (e.g., row or column vector)\n",
    "         if measurement.size == n_components:\n",
    "             measurement = measurement.flatten()\n",
    "         else:\n",
    "             raise ValueError(f\"Measurement shape {measurement.shape} cannot be flattened to match n_components {n_components}.\")\n",
    "    if measurement.ndim != 1 or measurement.shape[0] != n_components:\n",
    "         raise ValueError(f\"Measurement shape {measurement.shape} incompatible with n_components {n_components}. Expecting ({n_components},).\")\n",
    "    if lags_model.shape[0] != k_states:\n",
    "         raise ValueError(f\"lags_model length {lags_model.shape[0]} must match persistence length {k_states}.\")\n",
    "    if k_states != n_components:\n",
    "        raise ValueError(f\"Number of states ({k_states}) from persistence vector does not match transition matrix dimension ({n_components}). Check model definition.\")\n",
    "    # --- End Validation ---\n",
    "\n",
    "    # Prepare the persistence array (array_persistence_q)\n",
    "    # This array stores diagonal persistence matrices sliced according to steps\n",
    "    array_persistence_q = np.zeros((n_components, n_components, steps_number))\n",
    "    # Use the already flattened persistence array here\n",
    "    diag_matrix_full = np.diag(persistence) # Now guaranteed to be k_states x k_states\n",
    "\n",
    "    for i_step in range(steps_number):\n",
    "        mask = (lags_model == steps[i_step]) # Boolean array of length k_states\n",
    "        if np.sum(mask) > 0:\n",
    "            # Assign relevant columns from diag_matrix_full to the slice\n",
    "            # This indexing should now work correctly\n",
    "            array_persistence_q[:, mask, i_step] = diag_matrix_full[:, mask]\n",
    "\n",
    "    ## The matrices that will be used in the loop\n",
    "    matrix_persistence_q = np.zeros((n_components, n_components))\n",
    "    iq = np.zeros(1) # Accumulator, use array element for direct update\n",
    "    ik = np.eye(k_states) # Identity matrix\n",
    "\n",
    "    # The vector of variances, initialized to zeros (like R's rep(0,h))\n",
    "    var_mat = np.zeros(h)\n",
    "\n",
    "    # Calculate log variances for steps 2 to h (Python indices 1 to h-1)\n",
    "    if h > 1:\n",
    "        # Outer loop: R i goes 2..h. Python i goes 1..h-1. (Py_i = R_i - 1)\n",
    "        for i in range(1, h): # Corresponds to h steps 2, 3, ..., h\n",
    "            iq[0] = 0.0 # Reset accumulator for R step i+1\n",
    "            # R's inner loop limit: sum(steps < i), where i is R index (2..h)\n",
    "            # Python equivalent limit: sum(steps < Py_i + 1) = sum(steps < i + 1)\n",
    "            num_inner_loops = np.sum(steps < (i + 1))\n",
    "\n",
    "            # Inner loop: R k goes 1..num_inner_loops. Python k_idx goes 0..num_inner_loops-1. (Py_k_idx = R_k - 1)\n",
    "            for k_idx in range(num_inner_loops):\n",
    "                # Get persistence slice corresponding to the k_idx-th step (< i+1)\n",
    "                matrix_persistence_q[:] = array_persistence_q[:, :, k_idx]\n",
    "\n",
    "                # Get the k_idx-th unique lag overall (used in power calculation)\n",
    "                # R code uses lagsUnique[k], which maps to lags_unique[k_idx]\n",
    "                current_lag = lags_unique[k_idx]\n",
    "                if current_lag <= 0:\n",
    "                    raise ValueError(f\"Invalid lag found in lags_unique: {current_lag}\")\n",
    "\n",
    "                # Calculate the power exponent using the *correct* step index\n",
    "                # R uses ceiling(i / lagsUnique[k]) - 1, where i is R index (2..h)\n",
    "                # Python needs ceiling((Py_i + 1) / lags_unique[k_idx]) - 1\n",
    "                #             = ceiling((i + 1) / lags_unique[k_idx]) - 1\n",
    "                power_val = np.ceil((i + 1) / current_lag) - 1\n",
    "                power_val = int(power_val) # Ensure integer for matrix_power\n",
    "\n",
    "                # Perform the matrix calculations\n",
    "                try:\n",
    "                    mat_pers_q_pow2 = matrix_power_wrap(matrix_persistence_q, 2)\n",
    "                    term1 = mat_pers_q_pow2 * s2\n",
    "                    term2 = matrix_power_wrap(ik + term1, power_val)\n",
    "                    term3 = term2 - ik\n",
    "                    iq[0] += np.sum(np.diag(term3)) # Accumulate sum of diagonal\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in var_anal calculation for h={i+1}, k_idx={k_idx}, lag={current_lag}, power={power_val}: {e}\")\n",
    "                    iq[0] = np.nan # Propagate error as NaN\n",
    "                    break # Exit inner loop for this step i\n",
    "\n",
    "            # Assign log(iq) to the variance matrix (index i corresponds to R's i+1 step)\n",
    "            if np.isnan(iq[0]):\n",
    "                 var_mat[i] = np.nan\n",
    "            elif iq[0] <= 0:\n",
    "                 var_mat[i] = -np.inf if iq[0] == 0 else np.nan # Match R log behavior\n",
    "            else:\n",
    "                 var_mat[i] = np.log(iq[0])\n",
    "\n",
    "    # Final Adjustments - applied in the same order as R\n",
    "    # 1. Apply exp and multiply by (1 + s2)\n",
    "    var_mat = np.exp(var_mat) * (1 + s2)\n",
    "\n",
    "    # 2. Adjust the first element (index 0) - corresponds to R's varMat[1] adjustment\n",
    "    if h > 0:\n",
    "         # R: varMat[1] <- varMat[1] - 1. Initially exp(0)*(1+s2) -> 1+s2. Result s2.\n",
    "         var_mat[0] = var_mat[0] - 1\n",
    "\n",
    "    # 3. Adjust elements from index 1 onwards - corresponds to R's varMat[-1] adjustment\n",
    "    if h > 1:\n",
    "         # R: varMat[-1] <- varMat[-1] + s2 (elements 2..h)\n",
    "         # Python: elements 1..h-1\n",
    "         var_mat[1:] = var_mat[1:] + s2\n",
    "\n",
    "    # Optional: Replace any remaining non-finite values with NaN\n",
    "    var_mat[~np.isfinite(var_mat)] = np.nan\n",
    "\n",
    "    return var_mat\n",
    "# --- Make sure you have matrix_power_wrap defined ---\n",
    "def matrix_power_wrap(matrix, power):\n",
    "    \"\"\"\n",
    "    Helper function to compute matrix power. Handles integer powers >= 0.\n",
    "    \"\"\"\n",
    "    power = int(power)\n",
    "    if power < 0:\n",
    "        raise ValueError(f\"Matrix power calculation received negative power: {power}\")\n",
    "    elif power == 0:\n",
    "        return np.eye(matrix.shape[0])\n",
    "    else:\n",
    "        if matrix.ndim != 2 or matrix.shape[0] != matrix.shape[1]:\n",
    "             raise ValueError(f\"Matrix must be square for matrix power. Got shape: {matrix.shape}\")\n",
    "        if not np.isfinite(matrix).all():\n",
    "            # Or handle differently if necessary\n",
    "            raise ValueError(\"Matrix contains non-finite values.\")\n",
    "        try:\n",
    "            # Use numpy's matrix_power for integer exponents\n",
    "            return np.linalg.matrix_power(matrix, power)\n",
    "        except np.linalg.LinAlgError as e:\n",
    "             raise np.linalg.LinAlgError(f\"Numpy matrix_power failed for power {power}: {e}\") from e\n",
    "        except ValueError as e: # Catch other potential numpy errors\n",
    "             raise ValueError(f\"Error during numpy matrix_power for power {power}: {e}\") from e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.var_covar import sigma, covar_anal, var_anal\n",
    "import stats\n",
    "from scipy import stats\n",
    "from scipy.special import gamma\n",
    "import numpy as np\n",
    "\n",
    "# params info -> adam.params_info\n",
    "\n",
    "def generate_prediction_interval(predictions, \n",
    "                                 mat_wt, \n",
    "                                 mat_f, \n",
    "                                 vec_g, \n",
    "                                 prepared_model, \n",
    "                                 general, \n",
    "                                 observations_dict,\n",
    "                                 model_type_dict,\n",
    "                                 lags_dict,\n",
    "                                params_info, level):\n",
    "    \n",
    "\n",
    "    # stimate sigma\n",
    "    s2 = sigma(observations_dict, params_info, general, prepared_model)**2\n",
    "\n",
    "    # lines 8015 to 8022\n",
    "    # line 8404 -> I dont get the (is.scale(object$scale))\n",
    "    # Skipping for now.\n",
    "    # Will ask Ivan what this is \n",
    "\n",
    "    # Check if model is ETS and has certain distributions with multiplicative errors\n",
    "    if (model_type_dict['ets_model'] and \n",
    "        general['distribution'] in ['dinvgauss', 'dgamma', 'dlnorm', 'dllaplace', 'dls', 'dlgnorm'] and \n",
    "        model_type_dict['error_type'] == 'M'):\n",
    "\n",
    "        # again scale object\n",
    "        # lines 8425 8428\n",
    "\n",
    "        v_voc_multi = var_anal(lags_dict['lags_model_all'], general['h'], mat_wt[0], mat_f, vec_g, s2)\n",
    "\n",
    "        # Lines 8429-8433 in R/adam.R\n",
    "        # If distribution is one of the log-based ones, transform the variance\n",
    "        if general['distribution'] in ['dlnorm', 'dls', 'dllaplace', 'dlgnorm']:\n",
    "            v_voc_multi = np.log(1 + v_voc_multi)\n",
    "        \n",
    "        # Lines 8435-8437 in R/adam.R\n",
    "        # We don't do correct cumulatives in this case...\n",
    "        if general.get('cumulative', False):\n",
    "            v_voc_multi = np.sum(v_voc_multi)\n",
    "    else:\n",
    "        # Lines 8439-8441 in R/adam.R\n",
    "        v_voc_multi = covar_anal(lags_dict['lags_model_all'], general['h'], mat_wt, mat_f, vec_g, s2)\n",
    "        \n",
    "        # Skipping the is.scale check (lines 8442-8445)\n",
    "        \n",
    "        # Lines 8447-8453 in R/adam.R\n",
    "        # Do either the variance of sum, or a diagonal\n",
    "        if general.get('cumulative', False):\n",
    "            v_voc_multi = np.sum(v_voc_multi)\n",
    "        else:\n",
    "            v_voc_multi = np.diag(v_voc_multi)\n",
    "\n",
    "    # Extract extra values which we will include in the function call\n",
    "    # Now implement prediction intervals based on distribution\n",
    "    # Translating from R/adam.R lines 8515-8640\n",
    "    y_forecast = predictions\n",
    "    y_lower = np.zeros_like(y_forecast)\n",
    "    y_upper = np.zeros_like(y_forecast)\n",
    "\n",
    "    level_low = (1 - level) / 2\n",
    "    level_up = 1 - level_low\n",
    "    e_type = model_type_dict['error_type']  # \"A\" or \"M\"\n",
    "\n",
    "\n",
    "    distribution = general['distribution']\n",
    "    other_params = general.get('other', {}) # Handle cases where 'other' might be missing\n",
    "\n",
    "    if distribution == \"dnorm\":\n",
    "        scale = np.sqrt(v_voc_multi)\n",
    "        loc = 1 if e_type == \"M\" else 0\n",
    "        y_lower[:] = stats.norm.ppf(level_low, loc=loc, scale=scale)\n",
    "        y_upper[:] = stats.norm.ppf(level_up, loc=loc, scale=scale)\n",
    "\n",
    "    elif distribution == \"dlaplace\":\n",
    "        scale = np.sqrt(v_voc_multi / 2)\n",
    "        loc = 1 if e_type == \"M\" else 0\n",
    "        y_lower[:] = stats.laplace.ppf(level_low, loc=loc, scale=scale)\n",
    "        y_upper[:] = stats.laplace.ppf(level_up, loc=loc, scale=scale)\n",
    "\n",
    "    elif distribution == \"ds\":\n",
    "        # Assuming stats.s_dist exists and follows R's qs(p, location, scale) convention\n",
    "        # scale = (variance / 120)**0.25\n",
    "        scale = (v_voc_multi / 120)**0.25\n",
    "        loc = 1 if e_type == \"M\" else 0\n",
    "        try:\n",
    "            # Check if stats.s_dist exists before calling\n",
    "            if hasattr(stats, 's_dist') and hasattr(stats.s_dist, 'ppf'):\n",
    "                y_lower[:] = stats.s_dist.ppf(level_low, loc=loc, scale=scale)\n",
    "                y_upper[:] = stats.s_dist.ppf(level_up, loc=loc, scale=scale)\n",
    "            else:\n",
    "                print(\"Warning: stats.s_dist not found. Cannot calculate intervals for 'ds'.\")\n",
    "                y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating 'ds' interval: {e}\")\n",
    "            y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "    elif distribution == \"dgnorm\":\n",
    "        # stats.gennorm.ppf(q, beta, loc=0, scale=1)\n",
    "        shape_beta = other_params.get('shape')\n",
    "        if shape_beta is not None:\n",
    "            # Handle potential division by zero or issues with gamma function if shape is invalid\n",
    "            try:\n",
    "                scale = np.sqrt(v_voc_multi * (gamma(1 / shape_beta) / gamma(3 / shape_beta)))\n",
    "                loc = 1 if e_type == \"M\" else 0\n",
    "                y_lower[:] = stats.gennorm.ppf(level_low, beta=shape_beta, loc=loc, scale=scale)\n",
    "                y_upper[:] = stats.gennorm.ppf(level_up, beta=shape_beta, loc=loc, scale=scale)\n",
    "            except (ValueError, ZeroDivisionError) as e:\n",
    "                print(f\"Warning: Could not calculate scale for dgnorm (shape={shape_beta}). Error: {e}\")\n",
    "                y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "        else:\n",
    "            print(\"Warning: Shape parameter 'beta' not found for dgnorm.\")\n",
    "            y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "    elif distribution == \"dlogis\":\n",
    "        # Variance = (scale*pi)^2 / 3 => scale = sqrt(Variance*3) / pi\n",
    "        scale = np.sqrt(v_voc_multi * 3) / np.pi\n",
    "        loc = 1 if e_type == \"M\" else 0\n",
    "        y_lower[:] = stats.logistic.ppf(level_low, loc=loc, scale=scale)\n",
    "        y_upper[:] = stats.logistic.ppf(level_up, loc=loc, scale=scale)\n",
    "\n",
    "    elif distribution == \"dt\":\n",
    "        # stats.t.ppf(q, df, loc=0, scale=1)\n",
    "        df = adam.observations_dict['obs_in_sample'] - adam.params_info['n_param']\n",
    "        if df <= 0:\n",
    "            print(f\"Warning: Degrees of freedom ({df}) non-positive for dt distribution. Setting intervals to NaN.\")\n",
    "            y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "        else:\n",
    "            scale = np.sqrt(v_voc_multi)\n",
    "            if e_type == \"A\":\n",
    "                y_lower[:] = scale * stats.t.ppf(level_low, df)\n",
    "                y_upper[:] = scale * stats.t.ppf(level_up, df)\n",
    "            else: # Etype == \"M\"\n",
    "                y_lower[:] = (1 + scale * stats.t.ppf(level_low, df))\n",
    "                y_upper[:] = (1 + scale * stats.t.ppf(level_up, df))\n",
    "\n",
    "    elif distribution == \"dalaplace\":\n",
    "        # Assuming stats.alaplace exists: ppf(q, loc, scale, alpha or kappa)\n",
    "        alpha = other_params.get('alpha')\n",
    "        if alpha is not None and 0 < alpha < 1:\n",
    "            try:\n",
    "                # Scale parameter from R code\n",
    "                scale = np.sqrt(v_voc_multi * alpha**2 * (1 - alpha)**2 / (alpha**2 + (1 - alpha)**2))\n",
    "                loc = 1 if e_type == \"M\" else 0\n",
    "                # Assuming the third parameter is alpha/kappa\n",
    "                # Check if stats.alaplace exists before calling\n",
    "                if hasattr(stats, 'alaplace') and hasattr(stats.alaplace, 'ppf'):\n",
    "                    # SciPy <= 1.8 used 'kappa', >= 1.9 uses 'alpha'\n",
    "                    try:\n",
    "                        y_lower[:] = stats.alaplace.ppf(level_low, loc=loc, scale=scale, alpha=alpha)\n",
    "                        y_upper[:] = stats.alaplace.ppf(level_up, loc=loc, scale=scale, alpha=alpha)\n",
    "                    except TypeError: # Try kappa for older SciPy versions\n",
    "                        y_lower[:] = stats.alaplace.ppf(level_low, loc=loc, scale=scale, kappa=alpha)\n",
    "                        y_upper[:] = stats.alaplace.ppf(level_up, loc=loc, scale=scale, kappa=alpha)\n",
    "                else:\n",
    "                    print(\"Warning: stats.alaplace not found. Cannot calculate intervals for 'dalaplace'.\")\n",
    "                    y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "            except (ValueError, ZeroDivisionError) as e:\n",
    "                print(f\"Warning: Could not calculate scale for dalaplace (alpha={alpha}). Error: {e}\")\n",
    "                y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "        else:\n",
    "            print(f\"Warning: Alpha parameter ({alpha}) invalid or not found for dalaplace.\")\n",
    "            y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "    # Log-Distributions (handling depends on whether v_voc_multi is variance of log)\n",
    "    # Assuming v_voc_multi IS the variance of the log error based on R lines 8429-8433 if Etype=='M'\n",
    "    # For Etype=='A', R calculates these as if M and then adjusts. Python code does this too.\n",
    "\n",
    "    elif distribution == \"dlnorm\":\n",
    "        # stats.lognorm.ppf(q, s, loc=0, scale=1). s=sdlog, scale=exp(meanlog)\n",
    "        # Assuming E[1+err]=1 => meanlog = -sdlog^2/2 = -vcovMulti/2\n",
    "        sdlog = np.sqrt(v_voc_multi)\n",
    "        meanlog = -v_voc_multi / 2\n",
    "        scipy_scale = np.exp(meanlog)\n",
    "        # Calculate quantiles of (1+error) multiplier\n",
    "        y_lower_mult = stats.lognorm.ppf(level_low, s=sdlog, loc=0, scale=scipy_scale)\n",
    "        y_upper_mult = stats.lognorm.ppf(level_up, s=sdlog, loc=0, scale=scipy_scale)\n",
    "        # Final adjustment depends on Etype (handled AFTER this block in R/Python)\n",
    "\n",
    "\n",
    "    elif distribution == \"dllaplace\":\n",
    "        # Corresponds to exp(Laplace(0, b)) where b = sqrt(var_log/2)\n",
    "        scale_log = np.sqrt(v_voc_multi / 2)\n",
    "        # Calculate quantiles of (1+error) multiplier\n",
    "        y_lower_mult = np.exp(stats.laplace.ppf(level_low, loc=0, scale=scale_log))\n",
    "        y_upper_mult = np.exp(stats.laplace.ppf(level_up, loc=0, scale=scale_log))\n",
    "        # Final adjustment depends on Etype\n",
    "\n",
    "\n",
    "    elif distribution == \"dls\":\n",
    "        # Corresponds to exp(S(0, b)) where b = (var_log/120)**0.25\n",
    "        scale_log = (v_voc_multi / 120)**0.25\n",
    "        # Calculate quantiles of (1+error) multiplier\n",
    "        try:\n",
    "            # Check if stats.s_dist exists before calling\n",
    "            if hasattr(stats, 's_dist') and hasattr(stats.s_dist, 'ppf'):\n",
    "                y_lower_mult = np.exp(stats.s_dist.ppf(level_low, loc=0, scale=scale_log))\n",
    "                y_upper_mult = np.exp(stats.s_dist.ppf(level_up, loc=0, scale=scale_log))\n",
    "            else:\n",
    "                print(\"Warning: stats.s_dist not found. Cannot calculate intervals for 'dls'.\")\n",
    "                y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating 'dls' interval: {e}\")\n",
    "            y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "        # Final adjustment depends on Etype\n",
    "\n",
    "\n",
    "    elif distribution == \"dlgnorm\":\n",
    "        # Corresponds to exp(GenNorm(0, scale_log, beta))\n",
    "        shape_beta = other_params.get('shape')\n",
    "        if shape_beta is not None:\n",
    "            try:\n",
    "                scale_log = np.sqrt(v_voc_multi * (gamma(1 / shape_beta) / gamma(3 / shape_beta)))\n",
    "                # Calculate quantiles of (1+error) multiplier\n",
    "                y_lower_mult = np.exp(stats.gennorm.ppf(level_low, beta=shape_beta, loc=0, scale=scale_log))\n",
    "                y_upper_mult = np.exp(stats.gennorm.ppf(level_up, beta=shape_beta, loc=0, scale=scale_log))\n",
    "            except (ValueError, ZeroDivisionError) as e:\n",
    "                print(f\"Warning: Could not calculate scale for dlgnorm (shape={shape_beta}). Error: {e}\")\n",
    "                y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "        else:\n",
    "            print(\"Warning: Shape parameter 'beta' not found for dlgnorm.\")\n",
    "            y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "        # Final adjustment depends on Etype\n",
    "\n",
    "    # Distributions naturally multiplicative (or treated as such for intervals)\n",
    "    elif distribution == \"dinvgauss\":\n",
    "        # stats.invgauss.ppf(q, mu, loc=0, scale=1). mu is shape parameter.\n",
    "        # R: qinvgauss(p, mean=1, dispersion=vcovMulti) -> implies lambda = 1/vcovMulti\n",
    "        # Map (mean=1, lambda=1/vcovMulti) to scipy's mu. Tentative: mu = 1/vcovMulti?\n",
    "        # Variance = mean^3 / lambda. If mean=1, Var = 1/lambda. If vcovMulti=Var -> lambda=1/vcovMulti\n",
    "        # Let's try mu = 1 / vcovMulti as the shape parameter `mu` for scipy\n",
    "        if np.any(v_voc_multi <= 0):\n",
    "            print(\"Warning: Non-positive variance for dinvgauss. Setting intervals to NaN.\")\n",
    "            y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "        else:\n",
    "            mu_shape = 1.0 / v_voc_multi # Tentative mapping\n",
    "            # Calculate quantiles of (1+error) multiplier (mean should be 1)\n",
    "            y_lower_mult = stats.invgauss.ppf(level_low, mu=mu_shape, loc=0, scale=1) # loc=0, scale=1 for standard form around mu\n",
    "            y_upper_mult = stats.invgauss.ppf(level_up, mu=mu_shape, loc=0, scale=1)\n",
    "            # Need to rescale ppf output? Let's assume R's mean=1 implies the output is already centered around 1. Needs verification.\n",
    "\n",
    "\n",
    "    elif distribution == \"dgamma\":\n",
    "        # stats.gamma.ppf(q, a, loc=0, scale=1). a=shape.\n",
    "        # R: qgamma(p, shape=1/vcovMulti, scale=vcovMulti) -> Mean = shape*scale = 1. Variance = shape*scale^2 = vcovMulti.\n",
    "        if np.any(v_voc_multi <= 0):\n",
    "            print(\"Warning: Non-positive variance for dgamma. Setting intervals to NaN.\")\n",
    "            y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "        else:\n",
    "            shape_a = 1.0 / v_voc_multi\n",
    "            scale_param = v_voc_multi\n",
    "            # Calculate quantiles of (1+error) multiplier (mean is 1)\n",
    "            y_lower_mult = stats.gamma.ppf(level_low, a=shape_a, loc=0, scale=scale_param)\n",
    "            y_upper_mult = stats.gamma.ppf(level_up, a=shape_a, loc=0, scale=scale_param)\n",
    "\n",
    "    else:\n",
    "        print(f\"Warning: Distribution '{distribution}' not recognized for interval calculation.\")\n",
    "        y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "    # Final adjustments based on Etype (as done in R lines 8632-8640)\n",
    "    # This part should come *after* the above block in your main script\n",
    "    needs_etype_A_adjustment = distribution in [\"dlnorm\", \"dllaplace\", \"dls\", \"dlgnorm\", \"dinvgauss\", \"dgamma\"]\n",
    "\n",
    "    if needs_etype_A_adjustment and e_type == \"A\":\n",
    "        # Calculated _mult quantiles assuming multiplicative form, adjust for additive\n",
    "        y_lower[:] = (y_lower_mult - 1) * y_forecast\n",
    "        y_upper[:] = (y_upper_mult - 1) * y_forecast\n",
    "    elif needs_etype_A_adjustment and e_type == \"M\":\n",
    "        # Assign the calculated multiplicative quantiles directly\n",
    "        y_lower[:] = y_lower_mult\n",
    "        y_upper[:] = y_upper_mult\n",
    "\n",
    "    return y_lower, y_upper\n",
    "\n",
    "\n",
    "\n",
    "    # assign to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>lower_0.025</th>\n",
       "      <th>upper_0.975</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1961-01-01</th>\n",
       "      <td>446.745880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-02-01</th>\n",
       "      <td>421.460877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-03-01</th>\n",
       "      <td>475.544201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-04-01</th>\n",
       "      <td>499.710761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-05-01</th>\n",
       "      <td>512.928279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-06-01</th>\n",
       "      <td>585.301208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-07-01</th>\n",
       "      <td>675.128957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-08-01</th>\n",
       "      <td>666.561737</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-09-01</th>\n",
       "      <td>553.760531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-10-01</th>\n",
       "      <td>491.464745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-11-01</th>\n",
       "      <td>419.377938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-12-01</th>\n",
       "      <td>463.966043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean  lower_0.025  upper_0.975\n",
       "1961-01-01  446.745880          NaN          NaN\n",
       "1961-02-01  421.460877          NaN          NaN\n",
       "1961-03-01  475.544201          NaN          NaN\n",
       "1961-04-01  499.710761          NaN          NaN\n",
       "1961-05-01  512.928279          NaN          NaN\n",
       "1961-06-01  585.301208          NaN          NaN\n",
       "1961-07-01  675.128957          NaN          NaN\n",
       "1961-08-01  666.561737          NaN          NaN\n",
       "1961-09-01  553.760531          NaN          NaN\n",
       "1961-10-01  491.464745          NaN          NaN\n",
       "1961-11-01  419.377938          NaN          NaN\n",
       "1961-12-01  463.966043          NaN          NaN"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stats\n",
    "from scipy import stats\n",
    "from scipy.special import gamma\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level is provided in the call \n",
    "level = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines 8007 to 8014\n",
    "# In the function I pass mat_wt from the forecaster\n",
    "\n",
    "# Estimate prediction matrices \n",
    "if adam.prepared_model['measurement'].shape[0] < adam.general['h']:\n",
    "    mat_wt = np.tile(adam.prepared_model['measurement'][-1], (adam.general['h'], 1))\n",
    "else:\n",
    "    mat_wt = adam.prepared_model['measurement'][-adam.general['h']:]\n",
    "\n",
    "vec_g = adam.prepared_model['persistence']\n",
    "mat_f = adam.prepared_model['transition']\n",
    "\n",
    "# stimate sigma\n",
    "s2 = sigma(adam.observations_dict, adam.params_info, adam.general, adam.prepared_model)**2\n",
    "\n",
    "\n",
    "# line 8404 -> I dont get the (is.scale(object$scale))\n",
    "# Skipping for now.\n",
    "# Will ask Ivan what this is \n",
    "# Check if model is ETS and has certain distributions with multiplicative errors\n",
    "if (adam.model_type_dict['ets_model'] and \n",
    "    adam.general['distribution'] in ['dinvgauss', 'dgamma', 'dlnorm', 'dllaplace', 'dls', 'dlgnorm'] and \n",
    "    adam.model_type_dict['error_type'] == 'M'):\n",
    "\n",
    "    # again scale object\n",
    "    # lines 8425 8428\n",
    "\n",
    "    v_voc_multi = var_anal(adam.lags_dict['lags_model_all'], adam.general['h'], mat_wt[0], mat_f, vec_g, s2)\n",
    "\n",
    "    # Lines 8429-8433 in R/adam.R\n",
    "    # If distribution is one of the log-based ones, transform the variance\n",
    "    if adam.general['distribution'] in ['dlnorm', 'dls', 'dllaplace', 'dlgnorm']:\n",
    "        v_voc_multi = np.log(1 + v_voc_multi)\n",
    "    \n",
    "    # Lines 8435-8437 in R/adam.R\n",
    "    # We don't do correct cumulatives in this case...\n",
    "    if adam.general.get('cumulative', False):\n",
    "        v_voc_multi = np.sum(v_voc_multi)\n",
    "else:\n",
    "    # Lines 8439-8441 in R/adam.R\n",
    "    v_voc_multi = covar_anal(adam.lags_dict['lags_model_all'], adam.general['h'], mat_wt[0], mat_f, vec_g, s2)\n",
    "    \n",
    "    # Skipping the is.scale check (lines 8442-8445)\n",
    "    \n",
    "    # Lines 8447-8453 in R/adam.R\n",
    "    # Do either the variance of sum, or a diagonal\n",
    "    if adam.general.get('cumulative', False):\n",
    "        v_voc_multi = np.sum(v_voc_multi)\n",
    "    else:\n",
    "        v_voc_multi = np.diag(v_voc_multi)\n",
    "\n",
    "\n",
    "# Extract extra values which we will include in the function call\n",
    "# Now implement prediction intervals based on distribution\n",
    "# Translating from R/adam.R lines 8515-8640\n",
    "y_forecast = adam.predictions['mean'].values\n",
    "y_lower = np.zeros_like(y_forecast)\n",
    "y_upper = np.zeros_like(y_forecast)\n",
    "\n",
    "level_low = (1 - level) / 2\n",
    "level_up = 1 - level_low\n",
    "e_type = adam.model_type_dict['error_type']  # \"A\" or \"M\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# level is provided in the call \n",
    "level = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'time_elapsed', 'holdout', 'y_fitted', 'residuals', 'states', 'profiles_recent_table', 'persistence', 'transition', 'measurement', 'phi', 'initial', 'initial_type', 'initial_estimated', 'orders', 'arma', 'constant', 'n_param', 'occurrence', 'formula', 'regressors', 'loss', 'loss_value', 'log_lik', 'distribution', 'scale', 'other', 'B', 'lags', 'lags_all', 'FI'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.prepared_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "distribution = adam.general['distribution']\n",
    "other_params = adam.general.get('other', {}) # Handle cases where 'other' might be missing\n",
    "\n",
    "if distribution == \"dnorm\":\n",
    "    scale = np.sqrt(v_voc_multi)\n",
    "    loc = 1 if e_type == \"M\" else 0\n",
    "    y_lower[:] = stats.norm.ppf(level_low, loc=loc, scale=scale)\n",
    "    y_upper[:] = stats.norm.ppf(level_up, loc=loc, scale=scale)\n",
    "\n",
    "elif distribution == \"dlaplace\":\n",
    "    scale = np.sqrt(v_voc_multi / 2)\n",
    "    loc = 1 if e_type == \"M\" else 0\n",
    "    y_lower[:] = stats.laplace.ppf(level_low, loc=loc, scale=scale)\n",
    "    y_upper[:] = stats.laplace.ppf(level_up, loc=loc, scale=scale)\n",
    "\n",
    "elif distribution == \"ds\":\n",
    "    # Assuming stats.s_dist exists and follows R's qs(p, location, scale) convention\n",
    "    # scale = (variance / 120)**0.25\n",
    "    scale = (v_voc_multi / 120)**0.25\n",
    "    loc = 1 if e_type == \"M\" else 0\n",
    "    try:\n",
    "        # Check if stats.s_dist exists before calling\n",
    "        if hasattr(stats, 's_dist') and hasattr(stats.s_dist, 'ppf'):\n",
    "             y_lower[:] = stats.s_dist.ppf(level_low, loc=loc, scale=scale)\n",
    "             y_upper[:] = stats.s_dist.ppf(level_up, loc=loc, scale=scale)\n",
    "        else:\n",
    "             print(\"Warning: stats.s_dist not found. Cannot calculate intervals for 'ds'.\")\n",
    "             y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating 'ds' interval: {e}\")\n",
    "        y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "elif distribution == \"dgnorm\":\n",
    "    # stats.gennorm.ppf(q, beta, loc=0, scale=1)\n",
    "    shape_beta = other_params.get('shape')\n",
    "    if shape_beta is not None:\n",
    "        # Handle potential division by zero or issues with gamma function if shape is invalid\n",
    "        try:\n",
    "             scale = np.sqrt(v_voc_multi * (gamma(1 / shape_beta) / gamma(3 / shape_beta)))\n",
    "             loc = 1 if e_type == \"M\" else 0\n",
    "             y_lower[:] = stats.gennorm.ppf(level_low, beta=shape_beta, loc=loc, scale=scale)\n",
    "             y_upper[:] = stats.gennorm.ppf(level_up, beta=shape_beta, loc=loc, scale=scale)\n",
    "        except (ValueError, ZeroDivisionError) as e:\n",
    "             print(f\"Warning: Could not calculate scale for dgnorm (shape={shape_beta}). Error: {e}\")\n",
    "             y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "    else:\n",
    "        print(\"Warning: Shape parameter 'beta' not found for dgnorm.\")\n",
    "        y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "elif distribution == \"dlogis\":\n",
    "    # Variance = (scale*pi)^2 / 3 => scale = sqrt(Variance*3) / pi\n",
    "    scale = np.sqrt(v_voc_multi * 3) / np.pi\n",
    "    loc = 1 if e_type == \"M\" else 0\n",
    "    y_lower[:] = stats.logistic.ppf(level_low, loc=loc, scale=scale)\n",
    "    y_upper[:] = stats.logistic.ppf(level_up, loc=loc, scale=scale)\n",
    "\n",
    "elif distribution == \"dt\":\n",
    "    # stats.t.ppf(q, df, loc=0, scale=1)\n",
    "    df = adam.observations_dict['obs_in_sample'] - adam.params_info['n_param']\n",
    "    if df <= 0:\n",
    "         print(f\"Warning: Degrees of freedom ({df}) non-positive for dt distribution. Setting intervals to NaN.\")\n",
    "         y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "    else:\n",
    "        scale = np.sqrt(v_voc_multi)\n",
    "        if e_type == \"A\":\n",
    "            y_lower[:] = scale * stats.t.ppf(level_low, df)\n",
    "            y_upper[:] = scale * stats.t.ppf(level_up, df)\n",
    "        else: # Etype == \"M\"\n",
    "            y_lower[:] = (1 + scale * stats.t.ppf(level_low, df))\n",
    "            y_upper[:] = (1 + scale * stats.t.ppf(level_up, df))\n",
    "\n",
    "elif distribution == \"dalaplace\":\n",
    "     # Assuming stats.alaplace exists: ppf(q, loc, scale, alpha or kappa)\n",
    "     alpha = other_params.get('alpha')\n",
    "     if alpha is not None and 0 < alpha < 1:\n",
    "         try:\n",
    "             # Scale parameter from R code\n",
    "             scale = np.sqrt(v_voc_multi * alpha**2 * (1 - alpha)**2 / (alpha**2 + (1 - alpha)**2))\n",
    "             loc = 1 if e_type == \"M\" else 0\n",
    "             # Assuming the third parameter is alpha/kappa\n",
    "             # Check if stats.alaplace exists before calling\n",
    "             if hasattr(stats, 'alaplace') and hasattr(stats.alaplace, 'ppf'):\n",
    "                  # SciPy <= 1.8 used 'kappa', >= 1.9 uses 'alpha'\n",
    "                  try:\n",
    "                      y_lower[:] = stats.alaplace.ppf(level_low, loc=loc, scale=scale, alpha=alpha)\n",
    "                      y_upper[:] = stats.alaplace.ppf(level_up, loc=loc, scale=scale, alpha=alpha)\n",
    "                  except TypeError: # Try kappa for older SciPy versions\n",
    "                      y_lower[:] = stats.alaplace.ppf(level_low, loc=loc, scale=scale, kappa=alpha)\n",
    "                      y_upper[:] = stats.alaplace.ppf(level_up, loc=loc, scale=scale, kappa=alpha)\n",
    "             else:\n",
    "                  print(\"Warning: stats.alaplace not found. Cannot calculate intervals for 'dalaplace'.\")\n",
    "                  y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "         except (ValueError, ZeroDivisionError) as e:\n",
    "              print(f\"Warning: Could not calculate scale for dalaplace (alpha={alpha}). Error: {e}\")\n",
    "              y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "     else:\n",
    "         print(f\"Warning: Alpha parameter ({alpha}) invalid or not found for dalaplace.\")\n",
    "         y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "# Log-Distributions (handling depends on whether v_voc_multi is variance of log)\n",
    "# Assuming v_voc_multi IS the variance of the log error based on R lines 8429-8433 if Etype=='M'\n",
    "# For Etype=='A', R calculates these as if M and then adjusts. Python code does this too.\n",
    "\n",
    "elif distribution == \"dlnorm\":\n",
    "    # stats.lognorm.ppf(q, s, loc=0, scale=1). s=sdlog, scale=exp(meanlog)\n",
    "    # Assuming E[1+err]=1 => meanlog = -sdlog^2/2 = -vcovMulti/2\n",
    "    sdlog = np.sqrt(v_voc_multi)\n",
    "    meanlog = -v_voc_multi / 2\n",
    "    scipy_scale = np.exp(meanlog)\n",
    "    # Calculate quantiles of (1+error) multiplier\n",
    "    y_lower_mult = stats.lognorm.ppf(level_low, s=sdlog, loc=0, scale=scipy_scale)\n",
    "    y_upper_mult = stats.lognorm.ppf(level_up, s=sdlog, loc=0, scale=scipy_scale)\n",
    "    # Final adjustment depends on Etype (handled AFTER this block in R/Python)\n",
    "\n",
    "\n",
    "elif distribution == \"dllaplace\":\n",
    "    # Corresponds to exp(Laplace(0, b)) where b = sqrt(var_log/2)\n",
    "    scale_log = np.sqrt(v_voc_multi / 2)\n",
    "    # Calculate quantiles of (1+error) multiplier\n",
    "    y_lower_mult = np.exp(stats.laplace.ppf(level_low, loc=0, scale=scale_log))\n",
    "    y_upper_mult = np.exp(stats.laplace.ppf(level_up, loc=0, scale=scale_log))\n",
    "    # Final adjustment depends on Etype\n",
    "\n",
    "\n",
    "elif distribution == \"dls\":\n",
    "    # Corresponds to exp(S(0, b)) where b = (var_log/120)**0.25\n",
    "    scale_log = (v_voc_multi / 120)**0.25\n",
    "    # Calculate quantiles of (1+error) multiplier\n",
    "    try:\n",
    "         # Check if stats.s_dist exists before calling\n",
    "         if hasattr(stats, 's_dist') and hasattr(stats.s_dist, 'ppf'):\n",
    "             y_lower_mult = np.exp(stats.s_dist.ppf(level_low, loc=0, scale=scale_log))\n",
    "             y_upper_mult = np.exp(stats.s_dist.ppf(level_up, loc=0, scale=scale_log))\n",
    "         else:\n",
    "             print(\"Warning: stats.s_dist not found. Cannot calculate intervals for 'dls'.\")\n",
    "             y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "    except Exception as e:\n",
    "         print(f\"Error calculating 'dls' interval: {e}\")\n",
    "         y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "    # Final adjustment depends on Etype\n",
    "\n",
    "\n",
    "elif distribution == \"dlgnorm\":\n",
    "    # Corresponds to exp(GenNorm(0, scale_log, beta))\n",
    "    shape_beta = other_params.get('shape')\n",
    "    if shape_beta is not None:\n",
    "        try:\n",
    "            scale_log = np.sqrt(v_voc_multi * (gamma(1 / shape_beta) / gamma(3 / shape_beta)))\n",
    "            # Calculate quantiles of (1+error) multiplier\n",
    "            y_lower_mult = np.exp(stats.gennorm.ppf(level_low, beta=shape_beta, loc=0, scale=scale_log))\n",
    "            y_upper_mult = np.exp(stats.gennorm.ppf(level_up, beta=shape_beta, loc=0, scale=scale_log))\n",
    "        except (ValueError, ZeroDivisionError) as e:\n",
    "             print(f\"Warning: Could not calculate scale for dlgnorm (shape={shape_beta}). Error: {e}\")\n",
    "             y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "    else:\n",
    "        print(\"Warning: Shape parameter 'beta' not found for dlgnorm.\")\n",
    "        y_lower_mult, y_upper_mult = np.nan, np.nan\n",
    "    # Final adjustment depends on Etype\n",
    "\n",
    "# Distributions naturally multiplicative (or treated as such for intervals)\n",
    "elif distribution == \"dinvgauss\":\n",
    "    # stats.invgauss.ppf(q, mu, loc=0, scale=1). mu is shape parameter.\n",
    "    # R: qinvgauss(p, mean=1, dispersion=vcovMulti) -> implies lambda = 1/vcovMulti\n",
    "    # Map (mean=1, lambda=1/vcovMulti) to scipy's mu. Tentative: mu = 1/vcovMulti?\n",
    "    # Variance = mean^3 / lambda. If mean=1, Var = 1/lambda. If vcovMulti=Var -> lambda=1/vcovMulti\n",
    "    # Let's try mu = 1 / vcovMulti as the shape parameter `mu` for scipy\n",
    "    if np.any(v_voc_multi <= 0):\n",
    "         print(\"Warning: Non-positive variance for dinvgauss. Setting intervals to NaN.\")\n",
    "         y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "    else:\n",
    "         mu_shape = 1.0 / v_voc_multi # Tentative mapping\n",
    "         # Calculate quantiles of (1+error) multiplier (mean should be 1)\n",
    "         y_lower_mult = stats.invgauss.ppf(level_low, mu=mu_shape, loc=0, scale=1) # loc=0, scale=1 for standard form around mu\n",
    "         y_upper_mult = stats.invgauss.ppf(level_up, mu=mu_shape, loc=0, scale=1)\n",
    "         # Need to rescale ppf output? Let's assume R's mean=1 implies the output is already centered around 1. Needs verification.\n",
    "\n",
    "\n",
    "elif distribution == \"dgamma\":\n",
    "    # stats.gamma.ppf(q, a, loc=0, scale=1). a=shape.\n",
    "    # R: qgamma(p, shape=1/vcovMulti, scale=vcovMulti) -> Mean = shape*scale = 1. Variance = shape*scale^2 = vcovMulti.\n",
    "    if np.any(v_voc_multi <= 0):\n",
    "         print(\"Warning: Non-positive variance for dgamma. Setting intervals to NaN.\")\n",
    "         y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "    else:\n",
    "        shape_a = 1.0 / v_voc_multi\n",
    "        scale_param = v_voc_multi\n",
    "        # Calculate quantiles of (1+error) multiplier (mean is 1)\n",
    "        y_lower_mult = stats.gamma.ppf(level_low, a=shape_a, loc=0, scale=scale_param)\n",
    "        y_upper_mult = stats.gamma.ppf(level_up, a=shape_a, loc=0, scale=scale_param)\n",
    "\n",
    "else:\n",
    "    print(f\"Warning: Distribution '{distribution}' not recognized for interval calculation.\")\n",
    "    y_lower[:], y_upper[:] = np.nan, np.nan\n",
    "\n",
    "\n",
    "# Final adjustments based on Etype (as done in R lines 8632-8640)\n",
    "# This part should come *after* the above block in your main script\n",
    "needs_etype_A_adjustment = distribution in [\"dlnorm\", \"dllaplace\", \"dls\", \"dlgnorm\", \"dinvgauss\", \"dgamma\"]\n",
    "\n",
    "if needs_etype_A_adjustment and e_type == \"A\":\n",
    "     # Calculated _mult quantiles assuming multiplicative form, adjust for additive\n",
    "     y_lower[:] = (y_lower_mult - 1) * y_forecast\n",
    "     y_upper[:] = (y_upper_mult - 1) * y_forecast\n",
    "elif needs_etype_A_adjustment and e_type == \"M\":\n",
    "      # Assign the calculated multiplicative quantiles directly\n",
    "      y_lower[:] = y_lower_mult\n",
    "      y_upper[:] = y_upper_mult\n",
    "# else: y_lower, y_upper were already calculated correctly for non-adjustment cases\n",
    "\n",
    "\n",
    "# The final step of adding y_forecast for Etype==\"A\" or multiplying for Etype==\"M\"\n",
    "# happens *after* this block, using the calculated y_lower/y_upper offsets/multipliers.\n",
    "# Example (should be outside this block):\n",
    "# if interval != \"none\":\n",
    "#     if e_type == \"A\":\n",
    "#         y_lower_final = y_forecast + y_lower\n",
    "#         y_upper_final = y_forecast + y_upper\n",
    "#     else: # e_type == \"M\"\n",
    "#         y_lower_final = y_forecast * y_lower # Assuming y_lower holds the multiplier\n",
    "#         y_upper_final = y_forecast * y_upper # Assuming y_upper holds the multiplier\n",
    "#     # ... handle inf/-inf/nan ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07927904, 1.08473758, 1.08988407, 1.09476841, 1.0994285 ,\n",
       "       1.10389389, 1.10818817, 1.11233052, 1.11633679, 1.12022025,\n",
       "       1.12399215, 1.12766211])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'time_elapsed', 'holdout', 'y_fitted', 'residuals', 'states', 'profiles_recent_table', 'persistence', 'transition', 'measurement', 'phi', 'initial', 'initial_type', 'initial_estimated', 'orders', 'arma', 'constant', 'n_param', 'occurrence', 'formula', 'regressors', 'loss', 'loss_value', 'log_lik', 'distribution', 'scale', 'other', 'B', 'lags', 'lags_all', 'FI'])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam.prepared_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "if adam.general[\"interval\"] != None:\n",
    "    # Create copies to store the final interval bounds\n",
    "    y_lower_final = y_lower.copy()\n",
    "    y_upper_final = y_upper.copy()\n",
    "\n",
    "    # 1. Make sensible values out of extreme quantiles (handle Inf/-Inf)\n",
    "    if not adam.general[\"cumulative\"]:\n",
    "        # Check level_low for 0% quantile\n",
    "        zero_lower_mask = (level_low == 0)\n",
    "        if np.any(zero_lower_mask):\n",
    "            if e_type == \"A\":\n",
    "                y_lower_final[zero_lower_mask] = -np.inf\n",
    "            else: # e_type == \"M\"\n",
    "                y_lower_final[zero_lower_mask] = 0.0\n",
    "\n",
    "        # Check level_up for 100% quantile\n",
    "        one_upper_mask = (level_up == 1)\n",
    "        if np.any(one_upper_mask):\n",
    "            y_upper_final[one_upper_mask] = np.inf\n",
    "    else: # cumulative = True (Dealing with a single value)\n",
    "        if e_type == \"A\" and np.any(level_low == 0):\n",
    "            y_lower_final[:] = -np.inf\n",
    "        elif e_type == \"M\" and np.any(level_low == 0):\n",
    "            y_lower_final[:] = 0.0\n",
    "\n",
    "        if np.any(level_up == 1):\n",
    "            y_upper_final[:] = np.inf\n",
    "\n",
    "    # 2. Substitute NaNs\n",
    "    nan_lower_mask = np.isnan(y_lower_final)\n",
    "    if np.any(nan_lower_mask):\n",
    "        replace_val = 0.0 if e_type == \"A\" else 1.0\n",
    "        y_lower_final[nan_lower_mask] = replace_val\n",
    "\n",
    "    nan_upper_mask = np.isnan(y_upper_final)\n",
    "    if np.any(nan_upper_mask):\n",
    "        replace_val = 0.0 if e_type == \"A\" else 1.0\n",
    "        y_upper_final[nan_upper_mask] = replace_val\n",
    "\n",
    "    # 3. Combine intervals with forecasts\n",
    "    if e_type == \"A\":\n",
    "        # y_lower/upper_final currently hold offsets, add forecast\n",
    "        y_lower_final = y_forecast + y_lower_final\n",
    "        y_upper_final = y_forecast + y_upper_final\n",
    "    else: # e_type == \"M\"\n",
    "        # y_lower/upper_final currently hold multipliers, multiply forecast\n",
    "        y_lower_final = y_forecast * y_lower_final\n",
    "        y_upper_final = y_forecast * y_upper_final\n",
    "\n",
    "    # 4. Occurrence Model Adjustments (if applicable)\n",
    "    # We skip this for now \n",
    "    # if adam.prepared_model['occurrence'] != None:\n",
    "    #     # Ensure p_forecast is a numpy array with the same shape as y_forecast for broadcasting\n",
    "    #     p_forecast_arr = np.array(p_forecast).reshape(y_forecast.shape)\n",
    "    #\n",
    "    #     # Handle NaNs in upper bound\n",
    "    #     # Using np.divide to handle potential division by zero if p_forecast is 0\n",
    "    #     nan_upper_occur_mask = np.isnan(y_upper_final)\n",
    "    #     if np.any(nan_upper_occur_mask):\n",
    "    #          fill_values = np.divide(y_forecast, p_forecast_arr,\n",
    "    #                                  out=np.full_like(y_forecast, np.nan), # Fill with NaN if p_forecast is 0\n",
    "    #                                  where=p_forecast_arr!=0)\n",
    "    #          y_upper_final = np.where(nan_upper_occur_mask, fill_values, y_upper_final)\n",
    "    #\n",
    "    #\n",
    "    #     # Handle NaNs in lower bound\n",
    "    #     nan_lower_occur_mask = np.isnan(y_lower_final)\n",
    "    #     if np.any(nan_lower_occur_mask):\n",
    "    #         y_lower_final[nan_lower_occur_mask] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([482.16346256, 457.17445154, 518.28804702, 547.06755689,\n",
       "       563.92797074, 646.11043037, 748.169926  , 741.43696567,\n",
       "       618.18325408, 550.54875843, 471.37750787, 523.19692767])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_upper_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# M-Competition Evaluation\n",
    "\n",
    "This notebook evaluates ADAM and ES models on M1 and M3 competition datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mcomp import M1, M3, load_m1, load_m3\n",
    "from smooth import ADAM, ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-metrics-header",
   "metadata": {},
   "source": [
    "## Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "error-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSE(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Scaled Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    mse = np.mean((holdout - forecast) ** 2)\n",
    "    scale = np.mean(np.diff(actuals) ** 2)\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return np.sqrt(mse / scale)\n",
    "\n",
    "def SAME(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Scaled Absolute Mean Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    ame = np.abs(np.mean(holdout - forecast))\n",
    "    scale = np.mean(np.abs(np.diff(actuals)))\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return ame / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded M1 dataset: 1001 series\n",
      "Loaded M3 dataset: 3003 series\n",
      "Total series: 4004\n",
      "M1: 1001 series\n",
      "M3: 3003 series\n"
     ]
    }
   ],
   "source": [
    "# Load M1 and M3 datasets\n",
    "m1 = load_m1()\n",
    "m3 = load_m3()\n",
    "\n",
    "# Combine datasets into a list\n",
    "datasets = []\n",
    "for idx in m1.keys():\n",
    "    datasets.append(m1[idx])\n",
    "for idx in m3.keys():\n",
    "    datasets.append(m3[idx])\n",
    "\n",
    "print(f\"Total series: {len(datasets)}\")\n",
    "print(f\"M1: {len(m1)} series\")\n",
    "print(f\"M3: {len(m3)} series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-header",
   "metadata": {},
   "source": [
    "## Define Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "methods-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: 6\n",
      "Datasets: 4004\n"
     ]
    }
   ],
   "source": [
    "# Method names\n",
    "methods_names = [\n",
    "    \"ADAM ETS Back\",\n",
    "    \"ADAM ETS Opt\", \n",
    "    \"ADAM ETS Two\",\n",
    "    \"ES Back\",\n",
    "    \"ES Opt\",\n",
    "    \"ES Two\"\n",
    "]\n",
    "\n",
    "methods_number = len(methods_names)\n",
    "dataset_length = len(datasets)\n",
    "\n",
    "print(f\"Methods: {methods_number}\")\n",
    "print(f\"Datasets: {dataset_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-functions-header",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_series(series, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate a single method on a single series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : MCompSeries\n",
    "        Series to evaluate\n",
    "    method_name : str\n",
    "        Name of the method to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (RMSSE, time_elapsed)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine lags based on period\n",
    "        period = series.period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "        else:\n",
    "            lags = [1]\n",
    "        \n",
    "        # Select model and initial based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=\"ZXZ\", lags=lags, initial=initial)\n",
    "        model.fit(series.x)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.predict(h=series.h)\n",
    "        forecast_values = forecasts['mean'].values\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate RMSSE\n",
    "        rmsse = RMSSE(series.xx, forecast_values, series.x)\n",
    "        same = SAME(series.xx, forecast_values, series.x)\n",
    "        \n",
    "        return (rmsse, same, time_elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "def evaluate_method_sequential(datasets, method_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a method on all datasets sequentially.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries\n",
    "    method_name : str\n",
    "        Name of the method\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Arrays of (RMSSE values, time values)\n",
    "    \"\"\"\n",
    "    n = len(datasets)\n",
    "    rmsse_values = np.full(n, np.nan)\n",
    "    same_values = np.full(n, np.nan)\n",
    "    time_values = np.full(n, np.nan)\n",
    "    \n",
    "    for i, series in enumerate(datasets):\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(f\"  {method_name}: {i + 1}/{n}\")\n",
    "        \n",
    "        rmsse, same, elapsed = evaluate_single_series(series, method_name)\n",
    "        rmsse_values[i] = rmsse\n",
    "        same_values[i] = same\n",
    "        time_values[i] = elapsed\n",
    "    \n",
    "    return rmsse_values, same_values, time_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation-header",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "This may take a while depending on the number of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-small-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, test on a small subset to make sure everything works\n",
    "test_datasets = datasets[:10]\n",
    "\n",
    "print(\"Testing on first 10 series...\")\n",
    "for method in methods_names[:2]:  # Test first 2 methods\n",
    "    rmsse_vals, same_vals, time_vals = evaluate_method_sequential(test_datasets, method, verbose=False)\n",
    "    print(f\"{method}: Mean RMSSE = {np.nanmean(rmsse_vals):.4f}, SAME = {np.nanmean(same_vals):.4f}, Time = {np.nanmean(time_vals):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initialize-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results array\n",
    "# Shape: (methods, datasets, metrics) where metrics = [RMSSE, Time]\n",
    "test_results = np.full((methods_number, dataset_length, 2), np.nan)\n",
    "\n",
    "print(f\"Results array shape: {test_results.shape}\")\n",
    "print(f\"Methods: {methods_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-full-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation (this will take a long time)\n",
    "# Uncomment the following lines to run the full evaluation\n",
    "\n",
    "for j, method_name in enumerate(methods_names):\n",
    "    print(f\"\\nEvaluating {method_name} ({j+1}/{methods_number})...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    rmsse_values, same_values, time_values = evaluate_method_sequential(datasets, method_name)\n",
    "    \n",
    "    test_results[j, :, 0] = rmsse_values\n",
    "    test_results[j, :, 1] = same_values\n",
    "    test_results[j, :, 2] = time_values\n",
    "    \n",
    "    total_time = time.time() - start\n",
    "    print(f\"  Completed in {total_time:.1f}s\")\n",
    "    print(f\"  Mean RMSSE: {np.nanmean(rmsse_values):.4f}\")\n",
    "    print(f\"  Mean SAME: {np.nanmean(same_values):.4f}\")\n",
    "    print(f\"  Mean Time per series: {np.nanmean(time_values):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarize-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Method': methods_names,\n",
    "    'Mean RMSSE': [np.nanmean(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Median RMSSE': [np.nanmedian(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Mean SAME': [np.nanmean(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Median SAME': [np.nanmedian(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Mean Time (s)': [np.nanmean(test_results[j, :, 2]) for j in range(methods_number)],\n",
    "    'Total Time (s)': [np.nansum(test_results[j, :, 2]) for j in range(methods_number)],\n",
    "    'Failed': [np.sum(np.isnan(test_results[j, :, 0])) for j in range(methods_number)]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-by-type",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results by series type\n",
    "series_types = [s.type for s in datasets]\n",
    "unique_types = list(set(series_types))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS BY SERIES TYPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for stype in unique_types:\n",
    "    mask = np.array([s.type == stype for s in datasets])\n",
    "    print(f\"\\n{stype.upper()} ({np.sum(mask)} series):\")\n",
    "    \n",
    "    for j, method in enumerate(methods_names):\n",
    "        rmsse_type = test_results[j, mask, 0]\n",
    "        print(f\"  {method}: Mean RMSSE = {np.nanmean(rmsse_type):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import datetime\n",
    "\n",
    "date_str = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "np.save(f'test_results_{date_str}.npy', test_results)\n",
    "summary.to_csv(f'test_summary_{date_str}.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to test_results_{date_str}.npy and test_summary_{date_str}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-series-header",
   "metadata": {},
   "source": [
    "## Single Series Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-series-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single series to see detailed output\n",
    "series = M3[2568]\n",
    "print(f\"Series: {series}\")\n",
    "print(f\"Training length: {len(series.x)}\")\n",
    "print(f\"Test length: {len(series.xx)}\")\n",
    "print(f\"Period: {series.period}\")\n",
    "\n",
    "# Fit model\n",
    "model = ES(model=\"MAM\", lags=[1, series.period], initial=\"optimal\")\n",
    "model.fit(series.x)\n",
    "\n",
    "print(\"\\n\" + str(model))\n",
    "\n",
    "# Forecast\n",
    "forecasts = model.predict(h=series.h)\n",
    "print(\"\\nForecasts vs Actuals:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Forecast': forecasts['mean'].values,\n",
    "    'Actual': series.xx,\n",
    "    'Error': forecasts['mean'].values - series.xx\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Calculate error metrics\n",
    "rmsse = RMSSE(series.xx, forecasts['mean'].values, series.x)\n",
    "print(f\"\\nRMSSE: {rmsse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smooth)",
   "language": "python",
   "name": "smooth"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

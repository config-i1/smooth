{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# M-Competition Evaluation\n",
    "\n",
    "This notebook evaluates ADAM and ES models on M1 and M3 competition datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mcomp import M1, M3, load_m1, load_m3\n",
    "from smooth import ADAM, ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-metrics-header",
   "metadata": {},
   "source": [
    "## Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "error-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSE(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Scaled Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    mse = np.mean((holdout - forecast) ** 2)\n",
    "    scale = np.mean(np.diff(actuals) ** 2)\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return np.sqrt(mse / scale)\n",
    "\n",
    "def SAME(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Scaled Absolute Mean Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    ame = np.abs(np.mean(holdout - forecast))\n",
    "    scale = np.mean(np.abs(np.diff(actuals)))\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return ame / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded M1 dataset: 1001 series\n",
      "Loaded M3 dataset: 3003 series\n",
      "Total series: 4004\n",
      "M1: 1001 series\n",
      "M3: 3003 series\n"
     ]
    }
   ],
   "source": [
    "# Load M1 and M3 datasets\n",
    "m1 = load_m1()\n",
    "m3 = load_m3()\n",
    "\n",
    "# Combine datasets into a list\n",
    "datasets = []\n",
    "for idx in m1.keys():\n",
    "    datasets.append(m1[idx])\n",
    "for idx in m3.keys():\n",
    "    datasets.append(m3[idx])\n",
    "\n",
    "print(f\"Total series: {len(datasets)}\")\n",
    "print(f\"M1: {len(m1)} series\")\n",
    "print(f\"M3: {len(m3)} series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-header",
   "metadata": {},
   "source": [
    "## Define Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "methods-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: 6\n",
      "Datasets: 4004\n"
     ]
    }
   ],
   "source": [
    "# Method names\n",
    "methods_names = [\n",
    "    \"ADAM ETS Back\",\n",
    "    \"ADAM ETS Opt\", \n",
    "    \"ADAM ETS Two\",\n",
    "    \"ES Back\",\n",
    "    \"ES Opt\",\n",
    "    \"ES Two\"\n",
    "]\n",
    "\n",
    "methods_number = len(methods_names)\n",
    "dataset_length = len(datasets)\n",
    "\n",
    "print(f\"Methods: {methods_number}\")\n",
    "print(f\"Datasets: {dataset_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-functions-header",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "evaluation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_series(series, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate a single method on a single series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : MCompSeries\n",
    "        Series to evaluate\n",
    "    method_name : str\n",
    "        Name of the method to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (RMSSE, SAME, time_elapsed)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine lags and model based on period\n",
    "        period = series.period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "            model_str = \"ZXZ\"  # Auto-select including seasonality\n",
    "        else:\n",
    "            lags = [1]\n",
    "            model_str = \"ZXN\"  # Auto-select without seasonality for non-seasonal data\n",
    "        \n",
    "        # Select model class based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=model_str, lags=lags, initial=initial)\n",
    "        model.fit(series.x)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.predict(h=series.h)\n",
    "        forecast_values = forecasts['mean'].values\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate RMSSE\n",
    "        rmsse = RMSSE(series.xx, forecast_values, series.x)\n",
    "        same = SAME(series.xx, forecast_values, series.x)\n",
    "        \n",
    "        return (rmsse, same, time_elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "def evaluate_method_sequential(datasets, method_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a method on all datasets sequentially.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries\n",
    "    method_name : str\n",
    "        Name of the method\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Arrays of (RMSSE values, time values)\n",
    "    \"\"\"\n",
    "    n = len(datasets)\n",
    "    rmsse_values = np.full(n, np.nan)\n",
    "    same_values = np.full(n, np.nan)\n",
    "    time_values = np.full(n, np.nan)\n",
    "    \n",
    "    for i, series in enumerate(datasets):\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(f\"  {method_name}: {i + 1}/{n}\")\n",
    "        \n",
    "        rmsse, same, elapsed = evaluate_single_series(series, method_name)\n",
    "        rmsse_values[i] = rmsse\n",
    "        same_values[i] = same\n",
    "        time_values[i] = elapsed\n",
    "    \n",
    "    return rmsse_values, same_values, time_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation-header",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "This may take a while depending on the number of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "run-small-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on first 10 series...\n",
      "ADAM ETS Back: Mean RMSSE = 5.9556, SAME = 6.4621, Time = 0.058s\n",
      "ADAM ETS Opt: Mean RMSSE = 6.4098, SAME = 6.9332, Time = 0.123s\n"
     ]
    }
   ],
   "source": [
    "# First, test on a small subset to make sure everything works\n",
    "test_datasets = datasets[:10]\n",
    "\n",
    "print(\"Testing on first 10 series...\")\n",
    "for method in methods_names[:2]:  # Test first 2 methods\n",
    "    rmsse_vals, same_vals, time_vals = evaluate_method_sequential(test_datasets, method, verbose=False)\n",
    "    print(f\"{method}: Mean RMSSE = {np.nanmean(rmsse_vals):.4f}, SAME = {np.nanmean(same_vals):.4f}, Time = {np.nanmean(time_vals):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initialize-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results array shape: (6, 4004, 3)\n",
      "Methods: ['ADAM ETS Back', 'ADAM ETS Opt', 'ADAM ETS Two', 'ES Back', 'ES Opt', 'ES Two']\n"
     ]
    }
   ],
   "source": [
    "# Initialize results array\n",
    "# Shape: (methods, datasets, metrics) where metrics = [RMSSE, SAME, Time]\n",
    "test_results = np.full((methods_number, dataset_length, 3), np.nan)\n",
    "\n",
    "print(f\"Results array shape: {test_results.shape}\")\n",
    "print(f\"Methods: {methods_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "run-full-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation sequentially (alternative to parallel)\n",
    "# Skip this cell if using parallel evaluation above\n",
    "\n",
    "# for j, method_name in enumerate(methods_names):\n",
    "#     print(f\"\\nEvaluating {method_name} ({j+1}/{methods_number})...\")\n",
    "#     start = time.time()\n",
    "#     \n",
    "#     rmsse_values, same_values, time_values = evaluate_method_sequential(datasets, method_name)\n",
    "#     \n",
    "#     test_results[j, :, 0] = rmsse_values\n",
    "#     test_results[j, :, 1] = same_values\n",
    "#     test_results[j, :, 2] = time_values\n",
    "#     \n",
    "#     total_time = time.time() - start\n",
    "#     print(f\"  Completed in {total_time:.1f}s\")\n",
    "#     print(f\"  Mean RMSSE: {np.nanmean(rmsse_values):.4f}\")\n",
    "#     print(f\"  Mean SAME: {np.nanmean(same_values):.4f}\")\n",
    "#     print(f\"  Mean Time per series: {np.nanmean(time_values):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "en8djiptws7",
   "metadata": {},
   "source": [
    "## Parallel Evaluation\n",
    "\n",
    "Run evaluation using all CPU cores for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "y8zr0q53z4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_task(args):\n",
    "    \"\"\"\n",
    "    Worker function for parallel evaluation.\n",
    "    Must be defined at module level for pickling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        (series_idx, series_data, method_name) where series_data is a dict\n",
    "        containing the series attributes needed for evaluation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (series_idx, method_name, rmsse, same, time_elapsed)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import time\n",
    "    from smooth import ADAM, ES\n",
    "    \n",
    "    series_idx, series_data, method_name = args\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Reconstruct series data\n",
    "        x = series_data['x']\n",
    "        xx = series_data['xx']\n",
    "        h = series_data['h']\n",
    "        period = series_data['period']\n",
    "        \n",
    "        # Determine lags and model based on period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "            model_str = \"ZXZ\"\n",
    "        else:\n",
    "            lags = [1]\n",
    "            model_str = \"ZXN\"\n",
    "        \n",
    "        # Select model class based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=model_str, lags=lags, initial=initial)\n",
    "        model.fit(x)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.predict(h=h)\n",
    "        forecast_values = forecasts['mean'].values\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        holdout = np.asarray(xx)\n",
    "        actuals = np.asarray(x)\n",
    "        \n",
    "        # RMSSE\n",
    "        mse = np.mean((holdout - forecast_values) ** 2)\n",
    "        scale = np.mean(np.diff(actuals) ** 2)\n",
    "        rmsse = np.sqrt(mse / scale) if scale != 0 else np.nan\n",
    "        \n",
    "        # SAME\n",
    "        ame = np.abs(np.mean(holdout - forecast_values))\n",
    "        scale_same = np.mean(np.abs(np.diff(actuals)))\n",
    "        same = ame / scale_same if scale_same != 0 else np.nan\n",
    "        \n",
    "        return (series_idx, method_name, rmsse, same, time_elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (series_idx, method_name, np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "def evaluate_parallel(datasets, methods_names, n_workers=None):\n",
    "    \"\"\"\n",
    "    Evaluate all methods on all datasets in parallel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries objects\n",
    "    methods_names : list\n",
    "        List of method names to evaluate\n",
    "    n_workers : int, optional\n",
    "        Number of parallel workers. Defaults to all CPU cores.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Results array of shape (n_methods, n_datasets, 3) containing\n",
    "        [RMSSE, SAME, time] for each method-dataset combination\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    n_methods = len(methods_names)\n",
    "    n_datasets = len(datasets)\n",
    "    \n",
    "    # Initialize results array\n",
    "    results = np.full((n_methods, n_datasets, 3), np.nan)\n",
    "    \n",
    "    # Prepare tasks: convert series to picklable dicts\n",
    "    tasks = []\n",
    "    for j, method_name in enumerate(methods_names):\n",
    "        for i, series in enumerate(datasets):\n",
    "            series_data = {\n",
    "                'x': np.asarray(series.x),\n",
    "                'xx': np.asarray(series.xx),\n",
    "                'h': series.h,\n",
    "                'period': series.period\n",
    "            }\n",
    "            tasks.append((i, series_data, method_name))\n",
    "    \n",
    "    print(f\"Starting parallel evaluation with {n_workers} workers...\")\n",
    "    print(f\"Total tasks: {len(tasks)} ({n_methods} methods × {n_datasets} series)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    completed = 0\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = {executor.submit(_evaluate_task, task): task for task in tasks}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            series_idx, method_name, rmsse, same, elapsed = result\n",
    "            \n",
    "            # Find method index\n",
    "            method_idx = methods_names.index(method_name)\n",
    "            \n",
    "            # Store results\n",
    "            results[method_idx, series_idx, 0] = rmsse\n",
    "            results[method_idx, series_idx, 1] = same\n",
    "            results[method_idx, series_idx, 2] = elapsed\n",
    "            \n",
    "            completed += 1\n",
    "            if completed % 1000 == 0:\n",
    "                elapsed_total = time.time() - start_time\n",
    "                rate = completed / elapsed_total\n",
    "                remaining = (len(tasks) - completed) / rate\n",
    "                print(f\"  Progress: {completed}/{len(tasks)} ({100*completed/len(tasks):.1f}%) - \"\n",
    "                      f\"ETA: {remaining/60:.1f} min\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCompleted in {total_time/60:.1f} minutes ({total_time:.1f}s)\")\n",
    "    print(f\"Average time per task: {total_time/len(tasks)*1000:.1f}ms\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "swpfq4ermj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 32\n",
      "Starting parallel evaluation with 32 workers...\n",
      "Total tasks: 24024 (6 methods × 4004 series)\n",
      "  Progress: 1000/24024 (4.2%) - ETA: 9.3 min\n",
      "  Progress: 2000/24024 (8.3%) - ETA: 6.3 min\n",
      "  Progress: 3000/24024 (12.5%) - ETA: 7.4 min\n",
      "  Progress: 4000/24024 (16.7%) - ETA: 8.0 min\n",
      "  Progress: 5000/24024 (20.8%) - ETA: 10.4 min\n",
      "  Progress: 6000/24024 (25.0%) - ETA: 9.4 min\n",
      "  Progress: 7000/24024 (29.1%) - ETA: 10.9 min\n",
      "  Progress: 8000/24024 (33.3%) - ETA: 12.0 min\n",
      "  Progress: 9000/24024 (37.5%) - ETA: 11.9 min\n",
      "  Progress: 10000/24024 (41.6%) - ETA: 10.5 min\n",
      "  Progress: 11000/24024 (45.8%) - ETA: 10.5 min\n",
      "  Progress: 12000/24024 (50.0%) - ETA: 10.4 min\n",
      "  Progress: 13000/24024 (54.1%) - ETA: 9.2 min\n",
      "  Progress: 14000/24024 (58.3%) - ETA: 7.9 min\n",
      "  Progress: 15000/24024 (62.4%) - ETA: 6.9 min\n",
      "  Progress: 16000/24024 (66.6%) - ETA: 6.0 min\n",
      "  Progress: 17000/24024 (70.8%) - ETA: 5.4 min\n",
      "  Progress: 18000/24024 (74.9%) - ETA: 4.5 min\n",
      "  Progress: 19000/24024 (79.1%) - ETA: 3.9 min\n",
      "  Progress: 20000/24024 (83.3%) - ETA: 3.3 min\n",
      "  Progress: 21000/24024 (87.4%) - ETA: 2.5 min\n",
      "  Progress: 22000/24024 (91.6%) - ETA: 1.6 min\n",
      "  Progress: 23000/24024 (95.7%) - ETA: 0.8 min\n",
      "  Progress: 24000/24024 (99.9%) - ETA: 0.0 min\n",
      "\n",
      "Completed in 20.6 minutes (1235.1s)\n",
      "Average time per task: 51.4ms\n",
      "\n",
      "Per-method summary:\n",
      "  ADAM ETS Back: RMSSE=2.0918, SAME=2.1100, Time=0.771s, Failed=37\n",
      "  ADAM ETS Opt: RMSSE=2.0825, SAME=2.0975, Time=2.122s, Failed=37\n",
      "  ADAM ETS Two: RMSSE=2.0825, SAME=2.0975, Time=2.137s, Failed=37\n",
      "  ES Back: RMSSE=2.0911, SAME=2.1112, Time=0.773s, Failed=37\n",
      "  ES Opt: RMSSE=2.0854, SAME=2.1017, Time=2.074s, Failed=37\n",
      "  ES Two: RMSSE=2.0854, SAME=2.1017, Time=2.076s, Failed=37\n"
     ]
    }
   ],
   "source": [
    "# Run parallel evaluation using all CPU cores\n",
    "# This is much faster than sequential evaluation\n",
    "\n",
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "# Run parallel evaluation\n",
    "test_results = evaluate_parallel(datasets, methods_names)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPer-method summary:\")\n",
    "for j, method in enumerate(methods_names):\n",
    "    rmsse_mean = np.nanmean(test_results[j, :, 0])\n",
    "    same_mean = np.nanmean(test_results[j, :, 1])\n",
    "    time_mean = np.nanmean(test_results[j, :, 2])\n",
    "    failed = np.sum(np.isnan(test_results[j, :, 0]))\n",
    "    print(f\"  {method}: RMSSE={rmsse_mean:.4f}, SAME={same_mean:.4f}, \"\n",
    "          f\"Time={time_mean:.3f}s, Failed={failed}\")\n",
    "\n",
    "np.save('2026-01-18-Mcomp-test.npy', test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarize-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([253, 254, 255, 256, 257, 516, 517, 518, 549, 551, 552, 604, 605,\n",
      "       606, 607, 608, 609, 610, 611, 612, 613, 614, 627, 628, 629, 630,\n",
      "       840, 841, 855, 858, 868, 873, 973, 974, 975, 976, 986]),)\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "       Method  Mean RMSSE  Median RMSSE  Maximum RMSSE  Mean SAME  Median SAME  Mean Time (s)  Total Time (s)  Failed\n",
      "ADAM ETS Back    2.091757      1.242270      50.258736   2.110042     1.084838       0.770531     3056.697307      37\n",
      " ADAM ETS Opt    2.082516      1.268069      51.616184   2.097454     1.102463       2.122107     8418.397695      37\n",
      " ADAM ETS Two    2.082516      1.268069      51.616184   2.097454     1.102463       2.136846     8476.869047      37\n",
      "      ES Back    2.091125      1.245573      50.258736   2.111166     1.079962       0.772810     3065.736076      37\n",
      "       ES Opt    2.085444      1.265223      51.616184   2.101748     1.098140       2.073597     8225.957457      37\n",
      "       ES Two    2.085444      1.265223      51.616184   2.101748     1.098140       2.075600     8233.905079      37\n"
     ]
    }
   ],
   "source": [
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Method': methods_names,\n",
    "    'Mean RMSSE': [np.nanmean(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Median RMSSE': [np.nanmedian(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Maximum RMSSE': [np.nanmax(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Mean SAME': [np.nanmean(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Median SAME': [np.nanmedian(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Mean Time (s)': [np.nanmean(test_results[j, :, 2]) for j in range(methods_number)],\n",
    "    'Total Time (s)': [np.nansum(test_results[j, :, 2]) for j in range(methods_number)],\n",
    "    'Failed': [np.sum(np.isnan(test_results[j, :, 0])) for j in range(methods_number)]\n",
    "})\n",
    "\n",
    "print(np.where(np.isnan(test_results[0, :, 0])))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-by-type",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results by series type\n",
    "series_types = [s.type for s in datasets]\n",
    "unique_types = list(set(series_types))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS BY SERIES TYPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for stype in unique_types:\n",
    "    mask = np.array([s.type == stype for s in datasets])\n",
    "    print(f\"\\n{stype.upper()} ({np.sum(mask)} series):\")\n",
    "    \n",
    "    for j, method in enumerate(methods_names):\n",
    "        rmsse_type = test_results[j, mask, 0]\n",
    "        print(f\"  {method}: Mean RMSSE = {np.nanmean(rmsse_type):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save as numpy array\n",
    "np.save(f'test_results_{date_str}.npy', test_results)\n",
    "\n",
    "# Save summary as CSV\n",
    "summary.to_csv(f'test_summary_{date_str}.csv', index=False)\n",
    "\n",
    "# Save complete results with metadata using joblib\n",
    "results_dict = {\n",
    "    'test_results': test_results,\n",
    "    'methods_names': methods_names,\n",
    "    'dataset_info': [(s.sn, s.type, s.period, len(s.x), s.h) for s in datasets],\n",
    "    'summary': summary\n",
    "}\n",
    "joblib.dump(results_dict, f'test_results_full_{date_str}.joblib')\n",
    "\n",
    "print(f\"Results saved:\")\n",
    "print(f\"  - test_results_{date_str}.npy (raw array)\")\n",
    "print(f\"  - test_summary_{date_str}.csv (summary table)\")\n",
    "print(f\"  - test_results_full_{date_str}.joblib (complete with metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-series-header",
   "metadata": {},
   "source": [
    "## Single Series Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-series-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series: MCompSeries(sn='T1167', n=116, h=18, type='monthly')\n",
      "Training length: 116\n",
      "Test length: 18\n",
      "Period: 12\n",
      "\n",
      "Time elapsed: 0.17 seconds\n",
      "Model estimated using ES() function: ETS(MAM)\n",
      "With optimal initialisation\n",
      "Distribution assumed in the model: Normal\n",
      "Loss function type: likelihood; Loss function value: 870.2151\n",
      "Persistence vector g:\n",
      " alpha   beta  gamma\n",
      "0.0484 0.0080 0.6323\n",
      "Sample size: 116\n",
      "Number of estimated parameters: 16\n",
      "Number of degrees of freedom: 100\n",
      "Information criteria:\n",
      "      AIC      AICc       BIC      BICc\n",
      "1772.4303 1777.9252 1816.4877 1829.5481\n",
      "\n",
      "Forecasts vs Actuals:\n",
      "        Forecast   Actual        Error\n",
      "0   11045.606724  11818.9  -773.293276\n",
      "1    7911.136866   7682.9   228.236866\n",
      "2    7433.207803   7462.9   -29.692197\n",
      "3   10480.661190  11368.6  -887.938810\n",
      "4   10535.226776  11271.6  -736.373224\n",
      "5    6608.593468   6597.9    10.693468\n",
      "6    7188.648825   8328.8 -1140.151175\n",
      "7   14266.373490  13201.7  1064.673490\n",
      "8    6544.223745   7064.2  -519.976255\n",
      "9   11923.871768  12857.0  -933.128232\n",
      "10   8027.309737   8063.0   -35.690263\n",
      "11   8000.850601   8673.8  -672.949399\n",
      "12  11252.987059  12748.5 -1495.512941\n",
      "13   8059.435769   7866.8   192.635769\n",
      "14   7572.330317   8310.7  -738.369683\n",
      "15  10676.515433  12541.6 -1865.084567\n",
      "16  10731.794589  12296.8 -1565.005411\n",
      "17   6731.706157   7294.0  -562.293843\n",
      "\n",
      "RMSSE: 0.2888\n",
      "\n",
      "SAME: 0.2365\n"
     ]
    }
   ],
   "source": [
    "# Test on a single series to see detailed output\n",
    "series = M3[2568]\n",
    "print(f\"Series: {series}\")\n",
    "print(f\"Training length: {len(series.x)}\")\n",
    "print(f\"Test length: {len(series.xx)}\")\n",
    "print(f\"Period: {series.period}\")\n",
    "\n",
    "# Fit model\n",
    "model = ES(model=\"MAM\", lags=[1, series.period], initial=\"optimal\")\n",
    "model.fit(series.x)\n",
    "\n",
    "print(\"\\n\" + str(model))\n",
    "\n",
    "# Forecast\n",
    "forecasts = model.predict(h=series.h)\n",
    "print(\"\\nForecasts vs Actuals:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Forecast': forecasts['mean'].values,\n",
    "    'Actual': series.xx,\n",
    "    'Error': forecasts['mean'].values - series.xx\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Calculate error metrics\n",
    "rmsse = RMSSE(series.xx, forecasts['mean'].values, series.x)\n",
    "print(f\"\\nRMSSE: {rmsse:.4f}\")\n",
    "\n",
    "same = SAME(series.xx, forecasts['mean'].values, series.x)\n",
    "print(f\"\\nSAME: {same:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smooth)",
   "language": "python",
   "name": "smooth"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

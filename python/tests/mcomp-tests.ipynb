{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# M-Competition Evaluation\n",
    "\n",
    "This notebook evaluates ADAM and ES models on M1 and M3 competition datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mcomp import M1, M3, load_m1, load_m3\n",
    "from smooth import ADAM, ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-metrics-header",
   "metadata": {},
   "source": [
    "## Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "error-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSE(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Scaled Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    mse = np.mean((holdout - forecast) ** 2)\n",
    "    scale = np.mean(np.diff(actuals) ** 2)\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return np.sqrt(mse / scale)\n",
    "\n",
    "def SAME(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Scaled Absolute Mean Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    ame = np.abs(np.mean(holdout - forecast))\n",
    "    scale = np.mean(np.abs(np.diff(actuals)))\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return ame / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded M1 dataset: 1001 series\n",
      "Loaded M3 dataset: 3003 series\n",
      "Total series: 4004\n",
      "M1: 1001 series\n",
      "M3: 3003 series\n"
     ]
    }
   ],
   "source": [
    "# Load M1 and M3 datasets\n",
    "m1 = load_m1()\n",
    "m3 = load_m3()\n",
    "\n",
    "# Combine datasets into a list\n",
    "datasets = []\n",
    "for idx in m1.keys():\n",
    "    datasets.append(m1[idx])\n",
    "for idx in m3.keys():\n",
    "    datasets.append(m3[idx])\n",
    "\n",
    "print(f\"Total series: {len(datasets)}\")\n",
    "print(f\"M1: {len(m1)} series\")\n",
    "print(f\"M3: {len(m3)} series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-header",
   "metadata": {},
   "source": [
    "## Define Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "methods-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: 6\n",
      "Datasets: 4004\n"
     ]
    }
   ],
   "source": [
    "# Method names\n",
    "methods_names = [\n",
    "    \"ADAM ETS Back\",\n",
    "    \"ADAM ETS Opt\", \n",
    "    \"ADAM ETS Two\",\n",
    "    \"ES Back\",\n",
    "    \"ES Opt\",\n",
    "    \"ES Two\"\n",
    "]\n",
    "\n",
    "methods_number = len(methods_names)\n",
    "dataset_length = len(datasets)\n",
    "\n",
    "print(f\"Methods: {methods_number}\")\n",
    "print(f\"Datasets: {dataset_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-functions-header",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "evaluation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_series(series, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate a single method on a single series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : MCompSeries\n",
    "        Series to evaluate\n",
    "    method_name : str\n",
    "        Name of the method to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (RMSSE, SAME, time_elapsed)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine lags and model based on period\n",
    "        period = series.period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "            model_str = \"ZXZ\"  # Auto-select including seasonality\n",
    "        else:\n",
    "            lags = [1]\n",
    "            model_str = \"ZXN\"  # Auto-select without seasonality for non-seasonal data\n",
    "        \n",
    "        # Select model class based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=model_str, lags=lags, initial=initial)\n",
    "        model.fit(series.x)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.predict(h=series.h)\n",
    "        forecast_values = forecasts['mean'].values\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate RMSSE\n",
    "        rmsse = RMSSE(series.xx, forecast_values, series.x)\n",
    "        same = SAME(series.xx, forecast_values, series.x)\n",
    "        \n",
    "        return (rmsse, same, time_elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "def evaluate_method_sequential(datasets, method_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a method on all datasets sequentially.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries\n",
    "    method_name : str\n",
    "        Name of the method\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Arrays of (RMSSE values, time values)\n",
    "    \"\"\"\n",
    "    n = len(datasets)\n",
    "    rmsse_values = np.full(n, np.nan)\n",
    "    same_values = np.full(n, np.nan)\n",
    "    time_values = np.full(n, np.nan)\n",
    "    \n",
    "    for i, series in enumerate(datasets):\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(f\"  {method_name}: {i + 1}/{n}\")\n",
    "        \n",
    "        rmsse, same, elapsed = evaluate_single_series(series, method_name)\n",
    "        rmsse_values[i] = rmsse\n",
    "        same_values[i] = same\n",
    "        time_values[i] = elapsed\n",
    "    \n",
    "    return rmsse_values, same_values, time_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation-header",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "This may take a while depending on the number of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "run-small-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on first 10 series...\n",
      "ADAM ETS Back: Mean RMSSE = 5.9556, SAME = 6.4621, Time = 0.059s\n",
      "ADAM ETS Opt: Mean RMSSE = 6.4098, SAME = 6.9332, Time = 0.126s\n"
     ]
    }
   ],
   "source": [
    "# First, test on a small subset to make sure everything works\n",
    "test_datasets = datasets[:10]\n",
    "\n",
    "print(\"Testing on first 10 series...\")\n",
    "for method in methods_names[:2]:  # Test first 2 methods\n",
    "    rmsse_vals, same_vals, time_vals = evaluate_method_sequential(test_datasets, method, verbose=False)\n",
    "    print(f\"{method}: Mean RMSSE = {np.nanmean(rmsse_vals):.4f}, SAME = {np.nanmean(same_vals):.4f}, Time = {np.nanmean(time_vals):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initialize-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results array shape: (6, 4004, 3)\n",
      "Methods: ['ADAM ETS Back', 'ADAM ETS Opt', 'ADAM ETS Two', 'ES Back', 'ES Opt', 'ES Two']\n"
     ]
    }
   ],
   "source": [
    "# Initialize results array\n",
    "# Shape: (methods, datasets, metrics) where metrics = [RMSSE, SAME, Time]\n",
    "test_results = np.full((methods_number, dataset_length, 3), np.nan)\n",
    "\n",
    "print(f\"Results array shape: {test_results.shape}\")\n",
    "print(f\"Methods: {methods_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "run-full-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation sequentially (alternative to parallel)\n",
    "# Skip this cell if using parallel evaluation above\n",
    "\n",
    "# for j, method_name in enumerate(methods_names):\n",
    "#     print(f\"\\nEvaluating {method_name} ({j+1}/{methods_number})...\")\n",
    "#     start = time.time()\n",
    "#     \n",
    "#     rmsse_values, same_values, time_values = evaluate_method_sequential(datasets, method_name)\n",
    "#     \n",
    "#     test_results[j, :, 0] = rmsse_values\n",
    "#     test_results[j, :, 1] = same_values\n",
    "#     test_results[j, :, 2] = time_values\n",
    "#     \n",
    "#     total_time = time.time() - start\n",
    "#     print(f\"  Completed in {total_time:.1f}s\")\n",
    "#     print(f\"  Mean RMSSE: {np.nanmean(rmsse_values):.4f}\")\n",
    "#     print(f\"  Mean SAME: {np.nanmean(same_values):.4f}\")\n",
    "#     print(f\"  Mean Time per series: {np.nanmean(time_values):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "en8djiptws7",
   "metadata": {},
   "source": [
    "## Parallel Evaluation\n",
    "\n",
    "Run evaluation using all CPU cores for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "y8zr0q53z4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_task(args):\n",
    "    \"\"\"\n",
    "    Worker function for parallel evaluation.\n",
    "    Must be defined at module level for pickling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        (series_idx, series_data, method_name) where series_data is a dict\n",
    "        containing the series attributes needed for evaluation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (series_idx, method_name, rmsse, same, time_elapsed)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import time\n",
    "    from smooth import ADAM, ES\n",
    "    \n",
    "    series_idx, series_data, method_name = args\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Reconstruct series data\n",
    "        x = series_data['x']\n",
    "        xx = series_data['xx']\n",
    "        h = series_data['h']\n",
    "        period = series_data['period']\n",
    "        \n",
    "        # Determine lags and model based on period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "            model_str = \"ZXZ\"\n",
    "        else:\n",
    "            lags = [1]\n",
    "            model_str = \"ZXN\"\n",
    "        \n",
    "        # Select model class based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=model_str, lags=lags, initial=initial)\n",
    "        model.fit(x)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.predict(h=h)\n",
    "        forecast_values = forecasts['mean'].values\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        holdout = np.asarray(xx)\n",
    "        actuals = np.asarray(x)\n",
    "        \n",
    "        # RMSSE\n",
    "        mse = np.mean((holdout - forecast_values) ** 2)\n",
    "        scale = np.mean(np.diff(actuals) ** 2)\n",
    "        rmsse = np.sqrt(mse / scale) if scale != 0 else np.nan\n",
    "        \n",
    "        # SAME\n",
    "        ame = np.abs(np.mean(holdout - forecast_values))\n",
    "        scale_same = np.mean(np.abs(np.diff(actuals)))\n",
    "        same = ame / scale_same if scale_same != 0 else np.nan\n",
    "        \n",
    "        return (series_idx, method_name, rmsse, same, time_elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (series_idx, method_name, np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "def evaluate_parallel(datasets, methods_names, n_workers=None):\n",
    "    \"\"\"\n",
    "    Evaluate all methods on all datasets in parallel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries objects\n",
    "    methods_names : list\n",
    "        List of method names to evaluate\n",
    "    n_workers : int, optional\n",
    "        Number of parallel workers. Defaults to all CPU cores.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Results array of shape (n_methods, n_datasets, 3) containing\n",
    "        [RMSSE, SAME, time] for each method-dataset combination\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    n_methods = len(methods_names)\n",
    "    n_datasets = len(datasets)\n",
    "    \n",
    "    # Initialize results array\n",
    "    results = np.full((n_methods, n_datasets, 3), np.nan)\n",
    "    \n",
    "    # Prepare tasks: convert series to picklable dicts\n",
    "    tasks = []\n",
    "    for j, method_name in enumerate(methods_names):\n",
    "        for i, series in enumerate(datasets):\n",
    "            series_data = {\n",
    "                'x': np.asarray(series.x),\n",
    "                'xx': np.asarray(series.xx),\n",
    "                'h': series.h,\n",
    "                'period': series.period\n",
    "            }\n",
    "            tasks.append((i, series_data, method_name))\n",
    "    \n",
    "    print(f\"Starting parallel evaluation with {n_workers} workers...\")\n",
    "    print(f\"Total tasks: {len(tasks)} ({n_methods} methods × {n_datasets} series)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    completed = 0\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = {executor.submit(_evaluate_task, task): task for task in tasks}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            series_idx, method_name, rmsse, same, elapsed = result\n",
    "            \n",
    "            # Find method index\n",
    "            method_idx = methods_names.index(method_name)\n",
    "            \n",
    "            # Store results\n",
    "            results[method_idx, series_idx, 0] = rmsse\n",
    "            results[method_idx, series_idx, 1] = same\n",
    "            results[method_idx, series_idx, 2] = elapsed\n",
    "            \n",
    "            completed += 1\n",
    "            if completed % 1000 == 0:\n",
    "                elapsed_total = time.time() - start_time\n",
    "                rate = completed / elapsed_total\n",
    "                remaining = (len(tasks) - completed) / rate\n",
    "                print(f\"  Progress: {completed}/{len(tasks)} ({100*completed/len(tasks):.1f}%) - \"\n",
    "                      f\"ETA: {remaining/60:.1f} min\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCompleted in {total_time/60:.1f} minutes ({total_time:.1f}s)\")\n",
    "    print(f\"Average time per task: {total_time/len(tasks)*1000:.1f}ms\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "swpfq4ermj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 32\n",
      "Starting parallel evaluation with 32 workers...\n",
      "Total tasks: 24024 (6 methods × 4004 series)\n",
      "  Progress: 1000/24024 (4.2%) - ETA: 9.8 min\n",
      "  Progress: 2000/24024 (8.3%) - ETA: 6.6 min\n",
      "  Progress: 3000/24024 (12.5%) - ETA: 7.7 min\n",
      "  Progress: 4000/24024 (16.7%) - ETA: 8.3 min\n",
      "  Progress: 5000/24024 (20.8%) - ETA: 10.9 min\n",
      "  Progress: 6000/24024 (25.0%) - ETA: 9.8 min\n",
      "  Progress: 7000/24024 (29.1%) - ETA: 11.3 min\n",
      "  Progress: 8000/24024 (33.3%) - ETA: 12.5 min\n",
      "  Progress: 9000/24024 (37.5%) - ETA: 12.4 min\n",
      "  Progress: 10000/24024 (41.6%) - ETA: 11.0 min\n",
      "  Progress: 11000/24024 (45.8%) - ETA: 10.9 min\n",
      "  Progress: 12000/24024 (50.0%) - ETA: 10.8 min\n",
      "  Progress: 13000/24024 (54.1%) - ETA: 9.5 min\n",
      "  Progress: 14000/24024 (58.3%) - ETA: 8.2 min\n",
      "  Progress: 15000/24024 (62.4%) - ETA: 7.2 min\n",
      "  Progress: 16000/24024 (66.6%) - ETA: 6.3 min\n",
      "  Progress: 17000/24024 (70.8%) - ETA: 5.7 min\n",
      "  Progress: 18000/24024 (74.9%) - ETA: 4.7 min\n",
      "  Progress: 19000/24024 (79.1%) - ETA: 4.1 min\n",
      "  Progress: 20000/24024 (83.3%) - ETA: 3.5 min\n",
      "  Progress: 21000/24024 (87.4%) - ETA: 2.7 min\n",
      "  Progress: 22000/24024 (91.6%) - ETA: 1.7 min\n",
      "  Progress: 23000/24024 (95.7%) - ETA: 0.9 min\n",
      "  Progress: 24000/24024 (99.9%) - ETA: 0.0 min\n",
      "\n",
      "Completed in 21.7 minutes (1303.5s)\n",
      "Average time per task: 54.3ms\n",
      "\n",
      "Per-method summary:\n",
      "  ADAM ETS Back: RMSSE=2.0856, SAME=2.1031, Time=0.789s, Failed=0\n",
      "  ADAM ETS Opt: RMSSE=2.0778, SAME=2.0918, Time=2.192s, Failed=5\n",
      "  ADAM ETS Two: RMSSE=2.0778, SAME=2.0918, Time=2.196s, Failed=5\n",
      "  ES Back: RMSSE=2.0850, SAME=2.1041, Time=0.803s, Failed=0\n",
      "  ES Opt: RMSSE=2.0802, SAME=2.0953, Time=2.249s, Failed=5\n",
      "  ES Two: RMSSE=2.0802, SAME=2.0953, Time=2.191s, Failed=5\n"
     ]
    }
   ],
   "source": [
    "# Run parallel evaluation using all CPU cores\n",
    "# This is much faster than sequential evaluation\n",
    "\n",
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "# Run parallel evaluation\n",
    "test_results = evaluate_parallel(datasets, methods_names)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPer-method summary:\")\n",
    "for j, method in enumerate(methods_names):\n",
    "    rmsse_mean = np.nanmean(test_results[j, :, 0])\n",
    "    same_mean = np.nanmean(test_results[j, :, 1])\n",
    "    time_mean = np.nanmean(test_results[j, :, 2])\n",
    "    failed = np.sum(np.isnan(test_results[j, :, 0]))\n",
    "    print(f\"  {method}: RMSSE={rmsse_mean:.4f}, SAME={same_mean:.4f}, \"\n",
    "          f\"Time={time_mean:.3f}s, Failed={failed}\")\n",
    "\n",
    "np.save('2026-01-18-Mcomp-test.npy', test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarize-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),)\n",
      "[[1.12487082 1.00234645 0.39160657]\n",
      " [       nan        nan        nan]\n",
      " [       nan        nan        nan]\n",
      " [1.12487082 1.00234645 0.40633726]\n",
      " [       nan        nan        nan]\n",
      " [       nan        nan        nan]]\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "       Method      Min       Q1     Mean      Med       Q£       Max  Mean SAME  Med SAME  Mean Time (s)  Failed\n",
      "ADAM ETS Back 0.018252 0.707183 2.085643 1.241253 2.547780 50.258736   2.103068  1.084022       0.789343       0\n",
      " ADAM ETS Opt 0.024155 0.695437 2.077820 1.268780 2.558607 51.616184   2.091812  1.105564       2.192269       5\n",
      " ADAM ETS Two 0.024155 0.695437 2.077820 1.268780 2.558607 51.616184   2.091812  1.105564       2.195508       5\n",
      "      ES Back 0.018252 0.705564 2.084956 1.244262 2.540733 50.258736   2.104058  1.079399       0.802729       0\n",
      "       ES Opt 0.024155 0.705245 2.080214 1.265612 2.557251 51.616184   2.095257  1.100005       2.248851       5\n",
      "       ES Two 0.024155 0.705245 2.080214 1.265612 2.557251 51.616184   2.095257  1.100005       2.190780       5\n"
     ]
    }
   ],
   "source": [
    "test_results = np.load('2026-01-18-Mcomp-test.npy')\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Method': methods_names,\n",
    "    'Min': [np.nanmin(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Q1': [np.nanquantile(test_results[j, :, 0], 0.25) for j in range(methods_number)],\n",
    "    'Mean': [np.nanmean(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Med': [np.nanmedian(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Q3': [np.nanquantile(test_results[j, :, 0], 0.75) for j in range(methods_number)],\n",
    "    'Max': [np.nanmax(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Mean SAME': [np.nanmean(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Med SAME': [np.nanmedian(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Mean Time (s)': [np.nanmean(test_results[j, :, 2]) for j in range(methods_number)],\n",
    "    'Failed': [np.sum(np.isnan(test_results[j, :, 0])) for j in range(methods_number)]\n",
    "})\n",
    "\n",
    "print(np.where(np.isnan(test_results[0, :, 0])))\n",
    "\n",
    "print(test_results[:, 254, :])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.67 seconds\n",
      "Model estimated using ES() function: ETS(MAM)\n",
      "With backcasting initialisation\n",
      "Distribution assumed in the model: Normal\n",
      "Loss function type: likelihood; Loss function value: 869.8277\n",
      "Persistence vector g:\n",
      " alpha   beta  gamma\n",
      "0.0209 0.0208 0.0852\n",
      "Damping parameter: 1.0000\n",
      "Sample size: 116\n",
      "Number of estimated parameters: 3\n",
      "Number of degrees of freedom: 113\n",
      "Information criteria:\n",
      "      AIC      AICc       BIC      BICc\n",
      "1745.6554 1745.8697 1753.9161 1754.4254\n"
     ]
    }
   ],
   "source": [
    "series = M3[2569]\n",
    "\n",
    "# Fit model\n",
    "model = ES(model=\"ZXZ\", lags=[1, series.period], initial=\"backcasting\")\n",
    "model.fit(series.x)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "results-by-type",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULTS BY SERIES TYPE\n",
      "============================================================\n",
      "\n",
      "YEARLY (826 series):\n",
      "  ADAM ETS Back: Mean RMSSE = 2.6495\n",
      "  ADAM ETS Opt: Mean RMSSE = 2.6972\n",
      "  ADAM ETS Two: Mean RMSSE = 2.6972\n",
      "  ES Back: Mean RMSSE = 2.6533\n",
      "  ES Opt: Mean RMSSE = 2.6783\n",
      "  ES Two: Mean RMSSE = 2.6783\n",
      "\n",
      "MONTHLY (2045 series):\n",
      "  ADAM ETS Back: Mean RMSSE = 1.8684\n",
      "  ADAM ETS Opt: Mean RMSSE = 1.8497\n",
      "  ADAM ETS Two: Mean RMSSE = 1.8497\n",
      "  ES Back: Mean RMSSE = 1.8689\n",
      "  ES Opt: Mean RMSSE = 1.8614\n",
      "  ES Two: Mean RMSSE = 1.8614\n",
      "\n",
      "OTHER (174 series):\n",
      "  ADAM ETS Back: Mean RMSSE = 1.5346\n",
      "  ADAM ETS Opt: Mean RMSSE = 1.5135\n",
      "  ADAM ETS Two: Mean RMSSE = 1.5135\n",
      "  ES Back: Mean RMSSE = 1.5303\n",
      "  ES Opt: Mean RMSSE = 1.5021\n",
      "  ES Two: Mean RMSSE = 1.5021\n",
      "\n",
      "QUARTERLY (959 series):\n",
      "  ADAM ETS Back: Mean RMSSE = 2.1818\n",
      "  ADAM ETS Opt: Mean RMSSE = 2.1453\n",
      "  ADAM ETS Two: Mean RMSSE = 2.1453\n",
      "  ES Back: Mean RMSSE = 2.1757\n",
      "  ES Opt: Mean RMSSE = 2.1512\n",
      "  ES Two: Mean RMSSE = 2.1512\n"
     ]
    }
   ],
   "source": [
    "# Results by series type\n",
    "series_types = [s.type for s in datasets]\n",
    "unique_types = list(set(series_types))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS BY SERIES TYPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for stype in unique_types:\n",
    "    mask = np.array([s.type == stype for s in datasets])\n",
    "    print(f\"\\n{stype.upper()} ({np.sum(mask)} series):\")\n",
    "    \n",
    "    for j, method in enumerate(methods_names):\n",
    "        rmsse_type = test_results[j, mask, 0]\n",
    "        print(f\"  {method}: Mean RMSSE = {np.nanmean(rmsse_type):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save as numpy array\n",
    "np.save(f'test_results_{date_str}.npy', test_results)\n",
    "\n",
    "# Save summary as CSV\n",
    "summary.to_csv(f'test_summary_{date_str}.csv', index=False)\n",
    "\n",
    "# Save complete results with metadata using joblib\n",
    "results_dict = {\n",
    "    'test_results': test_results,\n",
    "    'methods_names': methods_names,\n",
    "    'dataset_info': [(s.sn, s.type, s.period, len(s.x), s.h) for s in datasets],\n",
    "    'summary': summary\n",
    "}\n",
    "joblib.dump(results_dict, f'test_results_full_{date_str}.joblib')\n",
    "\n",
    "print(f\"Results saved:\")\n",
    "print(f\"  - test_results_{date_str}.npy (raw array)\")\n",
    "print(f\"  - test_summary_{date_str}.csv (summary table)\")\n",
    "print(f\"  - test_results_full_{date_str}.joblib (complete with metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-series-header",
   "metadata": {},
   "source": [
    "## Single Series Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-series-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series: MCompSeries(sn='T72', n=49, h=8, type='quarterly')\n",
      "Training length: 49\n",
      "Test length: 8\n",
      "Period: 4\n",
      "\n",
      "Time elapsed: 0.44 seconds\n",
      "Model estimated using ES() function: ETS(ZXZ)\n",
      "With backcasting initialisation\n",
      "Distribution assumed in the model: Normal\n",
      "Loss function type: likelihood; Loss function value: 102.8211\n",
      "Persistence vector g:\n",
      " alpha   beta\n",
      "0.2968 0.0464\n",
      "Damping parameter: 1.0000\n",
      "Sample size: 49\n",
      "Number of estimated parameters: 2\n",
      "Number of degrees of freedom: 47\n",
      "Information criteria:\n",
      "      AIC      AICc       BIC      BICc\n",
      " 209.6423  209.9032  213.4259  213.9336\n",
      "\n",
      "Forecasts vs Actuals:\n",
      "     Forecast  Actual      Error\n",
      "0  170.798372   170.0   0.798372\n",
      "1  172.960023   171.0   1.960023\n",
      "2  175.121674   164.0  11.121674\n",
      "3  177.283326   162.0  15.283326\n",
      "4  179.444977   163.0  16.444977\n",
      "5  181.606628   163.0  18.606628\n",
      "6  183.768280   168.0  15.768280\n",
      "7  185.929931   174.0  11.929931\n",
      "\n",
      "RMSSE: 4.4282\n",
      "\n",
      "SAME: 5.3542\n"
     ]
    }
   ],
   "source": [
    "# Test on a single series to see detailed output\n",
    "series = M3[2568]\n",
    "print(f\"Series: {series}\")\n",
    "print(f\"Training length: {len(series.x)}\")\n",
    "print(f\"Test length: {len(series.xx)}\")\n",
    "print(f\"Period: {series.period}\")\n",
    "\n",
    "# Fit model\n",
    "model = ES(model=\"ZXZ\", lags=[1, series.period], initial=\"optimal\")\n",
    "model.fit(series.x)\n",
    "\n",
    "print(\"\\n\" + str(model))\n",
    "\n",
    "# Forecast\n",
    "forecasts = model.predict(h=series.h)\n",
    "print(\"\\nForecasts vs Actuals:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Forecast': forecasts['mean'].values,\n",
    "    'Actual': series.xx,\n",
    "    'Error': forecasts['mean'].values - series.xx\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Calculate error metrics\n",
    "rmsse = RMSSE(series.xx, forecasts['mean'].values, series.x)\n",
    "print(f\"\\nRMSSE: {rmsse:.4f}\")\n",
    "\n",
    "same = SAME(series.xx, forecasts['mean'].values, series.x)\n",
    "print(f\"\\nSAME: {same:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smooth)",
   "language": "python",
   "name": "smooth"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

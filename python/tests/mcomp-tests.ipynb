{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# M-Competition Evaluation\n",
    "\n",
    "This notebook evaluates ADAM and ES models on M1 and M3 competition datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from mcomp import M1, M3, load_m1, load_m3\n",
    "from smooth import ADAM, ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error-metrics-header",
   "metadata": {},
   "source": [
    "## Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "error-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSSE(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Scaled Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    mse = np.mean((holdout - forecast) ** 2)\n",
    "    scale = np.mean(np.diff(actuals) ** 2)\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return np.sqrt(mse / scale)\n",
    "\n",
    "def SAME(holdout, forecast, actuals):\n",
    "    \"\"\"\n",
    "    Scaled Absolute Mean Error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    holdout : array-like\n",
    "        Actual holdout values\n",
    "    forecast : array-like\n",
    "        Forecasted values\n",
    "    actuals : array-like\n",
    "        In-sample actual values (for scaling)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        RMSSE value\n",
    "    \"\"\"\n",
    "    holdout = np.asarray(holdout)\n",
    "    forecast = np.asarray(forecast)\n",
    "    actuals = np.asarray(actuals)\n",
    "    \n",
    "    ame = np.abs(np.mean(holdout - forecast))\n",
    "    scale = np.mean(np.abs(np.diff(actuals)))\n",
    "    \n",
    "    if scale == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return ame / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded M1 dataset: 1001 series\n",
      "Loaded M3 dataset: 3003 series\n",
      "Total series: 4004\n",
      "M1: 1001 series\n",
      "M3: 3003 series\n"
     ]
    }
   ],
   "source": [
    "# Load M1 and M3 datasets\n",
    "m1 = load_m1()\n",
    "m3 = load_m3()\n",
    "\n",
    "# Combine datasets into a list\n",
    "datasets = []\n",
    "for idx in m1.keys():\n",
    "    datasets.append(m1[idx])\n",
    "for idx in m3.keys():\n",
    "    datasets.append(m3[idx])\n",
    "\n",
    "print(f\"Total series: {len(datasets)}\")\n",
    "print(f\"M1: {len(m1)} series\")\n",
    "print(f\"M3: {len(m3)} series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "methods-header",
   "metadata": {},
   "source": [
    "## Define Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "methods-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methods: 6\n",
      "Datasets: 4004\n"
     ]
    }
   ],
   "source": [
    "# Method names\n",
    "methods_names = [\n",
    "    \"ADAM ETS Back\",\n",
    "    \"ADAM ETS Opt\", \n",
    "    \"ADAM ETS Two\",\n",
    "    \"ES Back\",\n",
    "    \"ES Opt\",\n",
    "    \"ES Two\"\n",
    "]\n",
    "\n",
    "methods_number = len(methods_names)\n",
    "dataset_length = len(datasets)\n",
    "\n",
    "print(f\"Methods: {methods_number}\")\n",
    "print(f\"Datasets: {dataset_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf4b01",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ADAM.fit() got an unexpected keyword argument 'print_level'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n",
      "\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ES(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAAdN\u001b[39m\u001b[38;5;124m\"\u001b[39m, lags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, series\u001b[38;5;241m.\u001b[39mperiod], initial\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackcasting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: ADAM.fit() got an unexpected keyword argument 'print_level'"
     ]
    }
   ],
   "source": [
    "series = datasets[349]\n",
    "\n",
    "# Fit model\n",
    "model = ES(model=\"AAdN\", lags=[1, series.period], initial=\"backcasting\", print_level=1)\n",
    "model.fit(series.x)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-functions-header",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "evaluation-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_series(series, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate a single method on a single series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : MCompSeries\n",
    "        Series to evaluate\n",
    "    method_name : str\n",
    "        Name of the method to use\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (RMSSE, SAME, time_elapsed)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine lags and model based on period\n",
    "        period = series.period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "            model_str = \"ZXZ\"  # Auto-select including seasonality\n",
    "        else:\n",
    "            lags = [1]\n",
    "            model_str = \"ZXN\"  # Auto-select without seasonality for non-seasonal data\n",
    "        \n",
    "        # Select model class based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=model_str, lags=lags, initial=initial)\n",
    "        model.fit(series.x)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.predict(h=series.h)\n",
    "        forecast_values = forecasts['mean'].values\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate RMSSE\n",
    "        rmsse = RMSSE(series.xx, forecast_values, series.x)\n",
    "        same = SAME(series.xx, forecast_values, series.x)\n",
    "        \n",
    "        return (rmsse, same, time_elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "def evaluate_method_sequential(datasets, method_name, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate a method on all datasets sequentially.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries\n",
    "    method_name : str\n",
    "        Name of the method\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Arrays of (RMSSE values, time values)\n",
    "    \"\"\"\n",
    "    n = len(datasets)\n",
    "    rmsse_values = np.full(n, np.nan)\n",
    "    same_values = np.full(n, np.nan)\n",
    "    time_values = np.full(n, np.nan)\n",
    "    \n",
    "    for i, series in enumerate(datasets):\n",
    "        if verbose and (i + 1) % 100 == 0:\n",
    "            print(f\"  {method_name}: {i + 1}/{n}\")\n",
    "        \n",
    "        rmsse, same, elapsed = evaluate_single_series(series, method_name)\n",
    "        rmsse_values[i] = rmsse\n",
    "        same_values[i] = same\n",
    "        time_values[i] = elapsed\n",
    "    \n",
    "    return rmsse_values, same_values, time_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-evaluation-header",
   "metadata": {},
   "source": [
    "## Run Evaluation\n",
    "\n",
    "This may take a while depending on the number of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "run-small-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on first 10 series...\n",
      "ADAM ETS Back: Mean RMSSE = 5.9556, SAME = 6.4621, Time = 0.059s\n",
      "ADAM ETS Opt: Mean RMSSE = 6.4098, SAME = 6.9332, Time = 0.126s\n"
     ]
    }
   ],
   "source": [
    "# First, test on a small subset to make sure everything works\n",
    "test_datasets = datasets[:10]\n",
    "\n",
    "print(\"Testing on first 10 series...\")\n",
    "for method in methods_names[:2]:  # Test first 2 methods\n",
    "    rmsse_vals, same_vals, time_vals = evaluate_method_sequential(test_datasets, method, verbose=False)\n",
    "    print(f\"{method}: Mean RMSSE = {np.nanmean(rmsse_vals):.4f}, SAME = {np.nanmean(same_vals):.4f}, Time = {np.nanmean(time_vals):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initialize-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results array shape: (6, 4004, 3)\n",
      "Methods: ['ADAM ETS Back', 'ADAM ETS Opt', 'ADAM ETS Two', 'ES Back', 'ES Opt', 'ES Two']\n"
     ]
    }
   ],
   "source": [
    "# Initialize results array\n",
    "# Shape: (methods, datasets, metrics) where metrics = [RMSSE, SAME, Time]\n",
    "test_results = np.full((methods_number, dataset_length, 3), np.nan)\n",
    "\n",
    "print(f\"Results array shape: {test_results.shape}\")\n",
    "print(f\"Methods: {methods_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "run-full-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation sequentially (alternative to parallel)\n",
    "# Skip this cell if using parallel evaluation above\n",
    "\n",
    "# for j, method_name in enumerate(methods_names):\n",
    "#     print(f\"\\nEvaluating {method_name} ({j+1}/{methods_number})...\")\n",
    "#     start = time.time()\n",
    "#     \n",
    "#     rmsse_values, same_values, time_values = evaluate_method_sequential(datasets, method_name)\n",
    "#     \n",
    "#     test_results[j, :, 0] = rmsse_values\n",
    "#     test_results[j, :, 1] = same_values\n",
    "#     test_results[j, :, 2] = time_values\n",
    "#     \n",
    "#     total_time = time.time() - start\n",
    "#     print(f\"  Completed in {total_time:.1f}s\")\n",
    "#     print(f\"  Mean RMSSE: {np.nanmean(rmsse_values):.4f}\")\n",
    "#     print(f\"  Mean SAME: {np.nanmean(same_values):.4f}\")\n",
    "#     print(f\"  Mean Time per series: {np.nanmean(time_values):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "en8djiptws7",
   "metadata": {},
   "source": [
    "## Parallel Evaluation\n",
    "\n",
    "Run evaluation using all CPU cores for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "y8zr0q53z4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_task(args):\n",
    "    \"\"\"\n",
    "    Worker function for parallel evaluation.\n",
    "    Must be defined at module level for pickling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        (series_idx, series_data, method_name) where series_data is a dict\n",
    "        containing the series attributes needed for evaluation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (series_idx, method_name, rmsse, same, time_elapsed)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import time\n",
    "    from smooth import ADAM, ES\n",
    "    \n",
    "    series_idx, series_data, method_name = args\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Reconstruct series data\n",
    "        x = series_data['x']\n",
    "        xx = series_data['xx']\n",
    "        h = series_data['h']\n",
    "        period = series_data['period']\n",
    "        \n",
    "        # Determine lags and model based on period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "            model_str = \"ZXZ\"\n",
    "        else:\n",
    "            lags = [1]\n",
    "            model_str = \"ZXN\"\n",
    "        \n",
    "        # Select model class based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=model_str, lags=lags, initial=initial)\n",
    "        model.fit(x)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecasts = model.predict(h=h)\n",
    "        forecast_values = forecasts['mean'].values\n",
    "        \n",
    "        time_elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        holdout = np.asarray(xx)\n",
    "        actuals = np.asarray(x)\n",
    "        \n",
    "        # RMSSE\n",
    "        mse = np.mean((holdout - forecast_values) ** 2)\n",
    "        scale = np.mean(np.diff(actuals) ** 2)\n",
    "        rmsse = np.sqrt(mse / scale) if scale != 0 else np.nan\n",
    "        \n",
    "        # SAME\n",
    "        ame = np.abs(np.mean(holdout - forecast_values))\n",
    "        scale_same = np.mean(np.abs(np.diff(actuals)))\n",
    "        same = ame / scale_same if scale_same != 0 else np.nan\n",
    "        \n",
    "        return (series_idx, method_name, rmsse, same, time_elapsed)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (series_idx, method_name, np.nan, np.nan, np.nan)\n",
    "\n",
    "\n",
    "def evaluate_parallel(datasets, methods_names, n_workers=None):\n",
    "    \"\"\"\n",
    "    Evaluate all methods on all datasets in parallel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries objects\n",
    "    methods_names : list\n",
    "        List of method names to evaluate\n",
    "    n_workers : int, optional\n",
    "        Number of parallel workers. Defaults to all CPU cores.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Results array of shape (n_methods, n_datasets, 3) containing\n",
    "        [RMSSE, SAME, time] for each method-dataset combination\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    n_methods = len(methods_names)\n",
    "    n_datasets = len(datasets)\n",
    "    \n",
    "    # Initialize results array\n",
    "    results = np.full((n_methods, n_datasets, 3), np.nan)\n",
    "    \n",
    "    # Prepare tasks: convert series to picklable dicts\n",
    "    tasks = []\n",
    "    for j, method_name in enumerate(methods_names):\n",
    "        for i, series in enumerate(datasets):\n",
    "            series_data = {\n",
    "                'x': np.asarray(series.x),\n",
    "                'xx': np.asarray(series.xx),\n",
    "                'h': series.h,\n",
    "                'period': series.period\n",
    "            }\n",
    "            tasks.append((i, series_data, method_name))\n",
    "    \n",
    "    print(f\"Starting parallel evaluation with {n_workers} workers...\")\n",
    "    print(f\"Total tasks: {len(tasks)} ({n_methods} methods × {n_datasets} series)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    completed = 0\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = {executor.submit(_evaluate_task, task): task for task in tasks}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            series_idx, method_name, rmsse, same, elapsed = result\n",
    "            \n",
    "            # Find method index\n",
    "            method_idx = methods_names.index(method_name)\n",
    "            \n",
    "            # Store results\n",
    "            results[method_idx, series_idx, 0] = rmsse\n",
    "            results[method_idx, series_idx, 1] = same\n",
    "            results[method_idx, series_idx, 2] = elapsed\n",
    "            \n",
    "            completed += 1\n",
    "            if completed % 1000 == 0:\n",
    "                elapsed_total = time.time() - start_time\n",
    "                rate = completed / elapsed_total\n",
    "                remaining = (len(tasks) - completed) / rate\n",
    "                print(f\"  Progress: {completed}/{len(tasks)} ({100*completed/len(tasks):.1f}%) - \"\n",
    "                      f\"ETA: {remaining/60:.1f} min\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCompleted in {total_time/60:.1f} minutes ({total_time:.1f}s)\")\n",
    "    print(f\"Average time per task: {total_time/len(tasks)*1000:.1f}ms\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "swpfq4ermj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available CPU cores: 32\n",
      "Starting parallel evaluation with 32 workers...\n",
      "Total tasks: 24024 (6 methods × 4004 series)\n",
      "  Progress: 1000/24024 (4.2%) - ETA: 9.8 min\n",
      "  Progress: 2000/24024 (8.3%) - ETA: 6.6 min\n",
      "  Progress: 3000/24024 (12.5%) - ETA: 7.7 min\n",
      "  Progress: 4000/24024 (16.7%) - ETA: 8.3 min\n",
      "  Progress: 5000/24024 (20.8%) - ETA: 10.9 min\n",
      "  Progress: 6000/24024 (25.0%) - ETA: 9.8 min\n",
      "  Progress: 7000/24024 (29.1%) - ETA: 11.3 min\n",
      "  Progress: 8000/24024 (33.3%) - ETA: 12.5 min\n",
      "  Progress: 9000/24024 (37.5%) - ETA: 12.4 min\n",
      "  Progress: 10000/24024 (41.6%) - ETA: 11.0 min\n",
      "  Progress: 11000/24024 (45.8%) - ETA: 10.9 min\n",
      "  Progress: 12000/24024 (50.0%) - ETA: 10.8 min\n",
      "  Progress: 13000/24024 (54.1%) - ETA: 9.5 min\n",
      "  Progress: 14000/24024 (58.3%) - ETA: 8.2 min\n",
      "  Progress: 15000/24024 (62.4%) - ETA: 7.2 min\n",
      "  Progress: 16000/24024 (66.6%) - ETA: 6.3 min\n",
      "  Progress: 17000/24024 (70.8%) - ETA: 5.7 min\n",
      "  Progress: 18000/24024 (74.9%) - ETA: 4.7 min\n",
      "  Progress: 19000/24024 (79.1%) - ETA: 4.1 min\n",
      "  Progress: 20000/24024 (83.3%) - ETA: 3.5 min\n",
      "  Progress: 21000/24024 (87.4%) - ETA: 2.7 min\n",
      "  Progress: 22000/24024 (91.6%) - ETA: 1.7 min\n",
      "  Progress: 23000/24024 (95.7%) - ETA: 0.9 min\n",
      "  Progress: 24000/24024 (99.9%) - ETA: 0.0 min\n",
      "\n",
      "Completed in 21.7 minutes (1303.5s)\n",
      "Average time per task: 54.3ms\n",
      "\n",
      "Per-method summary:\n",
      "  ADAM ETS Back: RMSSE=2.0856, SAME=2.1031, Time=0.789s, Failed=0\n",
      "  ADAM ETS Opt: RMSSE=2.0778, SAME=2.0918, Time=2.192s, Failed=5\n",
      "  ADAM ETS Two: RMSSE=2.0778, SAME=2.0918, Time=2.196s, Failed=5\n",
      "  ES Back: RMSSE=2.0850, SAME=2.1041, Time=0.803s, Failed=0\n",
      "  ES Opt: RMSSE=2.0802, SAME=2.0953, Time=2.249s, Failed=5\n",
      "  ES Two: RMSSE=2.0802, SAME=2.0953, Time=2.191s, Failed=5\n"
     ]
    }
   ],
   "source": [
    "# Run parallel evaluation using all CPU cores\n",
    "# This is much faster than sequential evaluation\n",
    "\n",
    "print(f\"Available CPU cores: {multiprocessing.cpu_count()}\")\n",
    "\n",
    "# Run parallel evaluation\n",
    "test_results = evaluate_parallel(datasets, methods_names)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nPer-method summary:\")\n",
    "for j, method in enumerate(methods_names):\n",
    "    rmsse_mean = np.nanmean(test_results[j, :, 0])\n",
    "    same_mean = np.nanmean(test_results[j, :, 1])\n",
    "    time_mean = np.nanmean(test_results[j, :, 2])\n",
    "    failed = np.sum(np.isnan(test_results[j, :, 0]))\n",
    "    print(f\"  {method}: RMSSE={rmsse_mean:.4f}, SAME={same_mean:.4f}, \"\n",
    "          f\"Time={time_mean:.3f}s, Failed={failed}\")\n",
    "\n",
    "np.save('2026-01-18-Mcomp-test.npy', test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "summarize-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "       Method      Min       Q1     Mean      Med       Q3       Max  Mean SAME  Med SAME  Mean Time (s)  Failed\n",
      "ADAM ETS Back 0.018252 0.707183 2.085643 1.241253 2.547780 50.258736   2.103068  1.084022       0.803426       0\n",
      " ADAM ETS Opt 0.024155 0.695568 2.078898 1.268761 2.558321 51.616184   2.092810  1.105280       2.241774       0\n",
      " ADAM ETS Two 0.024925 0.700068 2.093054 1.265177 2.532407 51.616184   2.110408  1.103959       2.409651       0\n",
      "      ES Back 0.018252 0.705564 2.084956 1.244262 2.540733 50.258736   2.104058  1.079399       0.813481       0\n",
      "       ES Opt 0.024155 0.706472 2.081472 1.266547 2.556744 51.616184   2.096443  1.101234       2.163924       0\n",
      "       ES Two 0.024925 0.711859 2.103972 1.273581 2.576052 51.616184   2.126635  1.105244       2.315632       0\n"
     ]
    }
   ],
   "source": [
    "test_results = np.load('2026-01-18-Mcomp-test.npy')\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary = pd.DataFrame({\n",
    "    'Method': methods_names,\n",
    "    'Min': [np.nanmin(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Q1': [np.nanquantile(test_results[j, :, 0], 0.25) for j in range(methods_number)],\n",
    "    'Mean': [np.nanmean(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Med': [np.nanmedian(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Q3': [np.nanquantile(test_results[j, :, 0], 0.75) for j in range(methods_number)],\n",
    "    'Max': [np.nanmax(test_results[j, :, 0]) for j in range(methods_number)],\n",
    "    'Mean SAME': [np.nanmean(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Med SAME': [np.nanmedian(test_results[j, :, 1]) for j in range(methods_number)],\n",
    "    'Mean Time (s)': [np.nanmean(test_results[j, :, 2]) for j in range(methods_number)],\n",
    "    'Failed': [np.sum(np.isnan(test_results[j, :, 0])) for j in range(methods_number)]\n",
    "})\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-by-type",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results by series type\n",
    "series_types = [s.type for s in datasets]\n",
    "unique_types = list(set(series_types))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS BY SERIES TYPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for stype in unique_types:\n",
    "    mask = np.array([s.type == stype for s in datasets])\n",
    "    print(f\"\\n{stype.upper()} ({np.sum(mask)} series):\")\n",
    "    \n",
    "    for j, method in enumerate(methods_names):\n",
    "        rmsse_type = test_results[j, mask, 0]\n",
    "        print(f\"  {method}: Mean RMSSE = {np.nanmean(rmsse_type):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import datetime\n",
    "import joblib\n",
    "\n",
    "date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save as numpy array\n",
    "np.save(f'test_results_{date_str}.npy', test_results)\n",
    "\n",
    "# Save summary as CSV\n",
    "summary.to_csv(f'test_summary_{date_str}.csv', index=False)\n",
    "\n",
    "# Save complete results with metadata using joblib\n",
    "results_dict = {\n",
    "    'test_results': test_results,\n",
    "    'methods_names': methods_names,\n",
    "    'dataset_info': [(s.sn, s.type, s.period, len(s.x), s.h) for s in datasets],\n",
    "    'summary': summary\n",
    "}\n",
    "joblib.dump(results_dict, f'test_results_full_{date_str}.joblib')\n",
    "\n",
    "print(f\"Results saved:\")\n",
    "print(f\"  - test_results_{date_str}.npy (raw array)\")\n",
    "print(f\"  - test_summary_{date_str}.csv (summary table)\")\n",
    "print(f\"  - test_results_full_{date_str}.joblib (complete with metadata)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-series-header",
   "metadata": {},
   "source": [
    "## Single Series Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52aeb09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.13 5.16 5.17 5.2  5.22 5.2  5.24 5.26 5.27 5.3  5.32 5.34 5.39 5.43\n",
      " 5.45 5.49 5.54 5.57 5.59 5.59 5.58 5.54 5.53 5.54 5.54 5.56 5.6  5.6\n",
      " 5.6  5.59 5.57 5.53 5.5  5.48 5.44 5.45 5.49 5.54 5.58 5.63 5.68 5.69\n",
      " 5.71 5.72 5.73 5.74 5.77 5.79 5.78 5.78 5.81 5.83 5.86 5.9  5.91 5.94]\n",
      "Time elapsed: 0.02 seconds\n",
      "Model estimated using ES() function: ETS(AAdN)\n",
      "With backcasting initialisation\n",
      "Distribution assumed in the model: Normal\n",
      "Loss function type: likelihood; Loss function value: -138.2533\n",
      "Persistence vector g:\n",
      " alpha   beta\n",
      "1.0000 0.6923\n",
      "Damping parameter: 1.0000\n",
      "Sample size: 56\n",
      "Number of estimated parameters: 4\n",
      "Number of degrees of freedom: 52\n",
      "Information criteria:\n",
      "      AIC      AICc       BIC      BICc\n",
      "-268.5066 -267.7222 -260.4052 -258.8266\n"
     ]
    }
   ],
   "source": [
    "series = datasets[349]\n",
    "print(series.x)\n",
    "\n",
    "# Fit model\n",
    "model = ES(model=\"AAdN\", lags=[1, series.period], initial=\"backcasting\")\n",
    "model.fit(series.x)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-series-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single series to see detailed output\n",
    "series = M3[203]\n",
    "print(f\"Series: {series}\")\n",
    "print(f\"Training length: {len(series.x)}\")\n",
    "print(f\"Test length: {len(series.xx)}\")\n",
    "print(f\"Period: {series.period}\")\n",
    "\n",
    "# Fit model\n",
    "model = ES(model=\"ZXZ\", lags=[1, series.period], initial=\"optimal\")\n",
    "model.fit(series.x)\n",
    "\n",
    "print(\"\\n\" + str(model))\n",
    "\n",
    "# Forecast\n",
    "forecasts = model.predict(h=series.h)\n",
    "print(\"\\nForecasts vs Actuals:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Forecast': forecasts['mean'].values,\n",
    "    'Actual': series.xx,\n",
    "    'Error': forecasts['mean'].values - series.xx\n",
    "})\n",
    "print(comparison)\n",
    "\n",
    "# Calculate error metrics\n",
    "rmsse = RMSSE(series.xx, forecasts['mean'].values, series.x)\n",
    "print(f\"\\nRMSSE: {rmsse:.4f}\")\n",
    "\n",
    "same = SAME(series.xx, forecasts['mean'].values, series.x)\n",
    "print(f\"\\nSAME: {same:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kobeicx8348",
   "metadata": {},
   "source": [
    "## Parameter Distribution Analysis\n",
    "\n",
    "Record model types, loss values, smoothing and dampening parameters for distribution analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "j16fmq1bsoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_params_task(args):\n",
    "    \"\"\"\n",
    "    Worker function for parallel parameter extraction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        (series_idx, series_data, method_name)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (series_idx, method_name, params_dict)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import time\n",
    "    from smooth import ADAM, ES\n",
    "    \n",
    "    series_idx, series_data, method_name = args\n",
    "    \n",
    "    # Initialize result dict with NaN/None defaults\n",
    "    params = {\n",
    "        'model_type': None,\n",
    "        'loss_value': np.nan,\n",
    "        'alpha': np.nan,\n",
    "        'beta': np.nan,\n",
    "        'gamma': np.nan,\n",
    "        'phi': np.nan,\n",
    "        'error': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Reconstruct series data\n",
    "        x = series_data['x']\n",
    "        period = series_data['period']\n",
    "        \n",
    "        # Determine lags and model based on period\n",
    "        if period > 1:\n",
    "            lags = [1, period]\n",
    "            model_str = \"ZXZ\"\n",
    "        else:\n",
    "            lags = [1]\n",
    "            model_str = \"ZXN\"\n",
    "        \n",
    "        # Select model class based on method\n",
    "        if \"ADAM\" in method_name:\n",
    "            model_class = ADAM\n",
    "        else:\n",
    "            model_class = ES\n",
    "        \n",
    "        if \"Back\" in method_name:\n",
    "            initial = \"backcasting\"\n",
    "        elif \"Opt\" in method_name:\n",
    "            initial = \"optimal\"\n",
    "        elif \"Two\" in method_name:\n",
    "            initial = \"two-stage\"\n",
    "        else:\n",
    "            initial = \"backcasting\"\n",
    "        \n",
    "        # Create and fit model\n",
    "        model = model_class(model=model_str, lags=lags, initial=initial)\n",
    "        model.fit(x)\n",
    "        \n",
    "        # Extract the SELECTED model type (not the input model string)\n",
    "        # Construct from error_type, trend_type, season_type, and damped flag\n",
    "        if hasattr(model, 'model_type_dict') and model.model_type_dict:\n",
    "            error_type = model.model_type_dict.get('error_type', '')\n",
    "            trend_type = model.model_type_dict.get('trend_type', '')\n",
    "            season_type = model.model_type_dict.get('season_type', '')\n",
    "            damped = model.model_type_dict.get('damped', False)\n",
    "            \n",
    "            # Construct model string: e.g., \"MAN\", \"AAdA\", \"ANN\"\n",
    "            if error_type and trend_type is not None and season_type is not None:\n",
    "                selected_model = error_type + trend_type\n",
    "                if damped and trend_type != 'N':\n",
    "                    selected_model += 'd'\n",
    "                selected_model += season_type\n",
    "                params['model_type'] = selected_model\n",
    "        \n",
    "        # Extract loss value\n",
    "        if hasattr(model, 'adam_estimated') and model.adam_estimated:\n",
    "            params['loss_value'] = model.adam_estimated.get('CF_value', np.nan)\n",
    "        \n",
    "        # Extract smoothing parameters\n",
    "        if hasattr(model, 'persistence_level_') and model.persistence_level_ is not None:\n",
    "            params['alpha'] = float(model.persistence_level_)\n",
    "        \n",
    "        if hasattr(model, 'persistence_trend_') and model.persistence_trend_ is not None:\n",
    "            params['beta'] = float(model.persistence_trend_)\n",
    "        \n",
    "        if hasattr(model, 'persistence_seasonal_') and model.persistence_seasonal_ is not None:\n",
    "            gamma = model.persistence_seasonal_\n",
    "            if isinstance(gamma, (list, np.ndarray)) and len(gamma) > 0:\n",
    "                params['gamma'] = float(gamma[0])\n",
    "            elif isinstance(gamma, (int, float)):\n",
    "                params['gamma'] = float(gamma)\n",
    "        \n",
    "        # Extract dampening parameter\n",
    "        if hasattr(model, 'phi_') and model.phi_ is not None:\n",
    "            params['phi'] = float(model.phi_)\n",
    "        \n",
    "        return (series_idx, method_name, params)\n",
    "    \n",
    "    except Exception as e:\n",
    "        params['error'] = str(e)\n",
    "        return (series_idx, method_name, params)\n",
    "\n",
    "\n",
    "def extract_params_parallel(datasets, methods_names, n_workers=None):\n",
    "    \"\"\"\n",
    "    Extract model parameters for all methods on all datasets in parallel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of MCompSeries objects\n",
    "    methods_names : list\n",
    "        List of method names to evaluate\n",
    "    n_workers : int, optional\n",
    "        Number of parallel workers. Defaults to all CPU cores.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Nested dictionary with structure:\n",
    "        {\n",
    "            method_name: {\n",
    "                'model_types': [...],      # list of selected model types (e.g., 'AAN', 'MAdM')\n",
    "                'loss_values': [...],      # list of loss values\n",
    "                'alpha': [...],            # list of alpha (level smoothing) values\n",
    "                'beta': [...],             # list of beta (trend smoothing) values\n",
    "                'gamma': [...],            # list of gamma (seasonal smoothing) values\n",
    "                'phi': [...],              # list of phi (dampening) values\n",
    "                'errors': [...],           # list of error messages (None if success)\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "    \n",
    "    n_methods = len(methods_names)\n",
    "    n_datasets = len(datasets)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {\n",
    "        method: {\n",
    "            'model_types': [None] * n_datasets,\n",
    "            'loss_values': [np.nan] * n_datasets,\n",
    "            'alpha': [np.nan] * n_datasets,\n",
    "            'beta': [np.nan] * n_datasets,\n",
    "            'gamma': [np.nan] * n_datasets,\n",
    "            'phi': [np.nan] * n_datasets,\n",
    "            'errors': [None] * n_datasets,\n",
    "        }\n",
    "        for method in methods_names\n",
    "    }\n",
    "    \n",
    "    # Prepare tasks\n",
    "    tasks = []\n",
    "    for method_name in methods_names:\n",
    "        for i, series in enumerate(datasets):\n",
    "            series_data = {\n",
    "                'x': np.asarray(series.x),\n",
    "                'period': series.period\n",
    "            }\n",
    "            tasks.append((i, series_data, method_name))\n",
    "    \n",
    "    print(f\"Starting parallel parameter extraction with {n_workers} workers...\")\n",
    "    print(f\"Total tasks: {len(tasks)} ({n_methods} methods × {n_datasets} series)\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    completed = 0\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "        futures = {executor.submit(_extract_params_task, task): task for task in tasks}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            series_idx, method_name, params = future.result()\n",
    "            \n",
    "            # Store results\n",
    "            results[method_name]['model_types'][series_idx] = params['model_type']\n",
    "            results[method_name]['loss_values'][series_idx] = params['loss_value']\n",
    "            results[method_name]['alpha'][series_idx] = params['alpha']\n",
    "            results[method_name]['beta'][series_idx] = params['beta']\n",
    "            results[method_name]['gamma'][series_idx] = params['gamma']\n",
    "            results[method_name]['phi'][series_idx] = params['phi']\n",
    "            results[method_name]['errors'][series_idx] = params['error']\n",
    "            \n",
    "            completed += 1\n",
    "            if completed % 1000 == 0:\n",
    "                elapsed_total = time.time() - start_time\n",
    "                rate = completed / elapsed_total\n",
    "                remaining = (len(tasks) - completed) / rate\n",
    "                print(f\"  Progress: {completed}/{len(tasks)} ({100*completed/len(tasks):.1f}%) - \"\n",
    "                      f\"ETA: {remaining/60:.1f} min\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nCompleted in {total_time/60:.1f} minutes ({total_time:.1f}s)\")\n",
    "    \n",
    "    # Convert lists to numpy arrays for easier analysis\n",
    "    for method in methods_names:\n",
    "        results[method]['loss_values'] = np.array(results[method]['loss_values'])\n",
    "        results[method]['alpha'] = np.array(results[method]['alpha'])\n",
    "        results[method]['beta'] = np.array(results[method]['beta'])\n",
    "        results[method]['gamma'] = np.array(results[method]['gamma'])\n",
    "        results[method]['phi'] = np.array(results[method]['phi'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8qovle0mmze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting parallel parameter extraction with 32 workers...\n",
      "Total tasks: 24024 (6 methods × 4004 series)\n",
      "  Progress: 1000/24024 (4.2%) - ETA: 9.8 min\n",
      "  Progress: 2000/24024 (8.3%) - ETA: 6.7 min\n",
      "  Progress: 3000/24024 (12.5%) - ETA: 7.8 min\n",
      "  Progress: 4000/24024 (16.7%) - ETA: 8.4 min\n",
      "  Progress: 5000/24024 (20.8%) - ETA: 11.1 min\n",
      "  Progress: 6000/24024 (25.0%) - ETA: 9.9 min\n",
      "  Progress: 7000/24024 (29.1%) - ETA: 11.4 min\n",
      "  Progress: 8000/24024 (33.3%) - ETA: 12.6 min\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Progress: 9000/24024 (37.5%) - ETA: 12.7 min\n",
      "\n",
      "\n",
      "  Progress: 10000/24024 (41.6%) - ETA: 11.2 min\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Progress: 11000/24024 (45.8%) - ETA: 11.2 min\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Progress: 12000/24024 (50.0%) - ETA: 11.2 min\n",
      "\n",
      "  Progress: 13000/24024 (54.1%) - ETA: 9.8 min\n",
      "  Progress: 14000/24024 (58.3%) - ETA: 8.4 min\n",
      "  Progress: 15000/24024 (62.4%) - ETA: 7.4 min\n",
      "  Progress: 16000/24024 (66.6%) - ETA: 6.4 min\n",
      "  Progress: 17000/24024 (70.8%) - ETA: 5.8 min\n",
      "  Progress: 18000/24024 (74.9%) - ETA: 4.8 min\n",
      "  Progress: 19000/24024 (79.1%) - ETA: 4.2 min\n",
      "  Progress: 20000/24024 (83.3%) - ETA: 3.5 min\n",
      "\n",
      "\n",
      "\n",
      "  Progress: 21000/24024 (87.4%) - ETA: 2.7 min\n",
      "\n",
      "\n",
      "\n",
      "  Progress: 22000/24024 (91.6%) - ETA: 1.7 min\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Progress: 23000/24024 (95.7%) - ETA: 0.9 min\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Progress: 24000/24024 (99.9%) - ETA: 0.0 min\n",
      "\n",
      "Completed in 22.0 minutes (1319.5s)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'joblib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2026-01-19-Mcomp-param_results.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, param_results)\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n",
      "\u001b[0;32m----> 7\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241m.\u001b[39mdump(param_results, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2026-01-19-Mcomp-params.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter results saved to 2026-01-19-Mcomp-params.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'joblib' is not defined"
     ]
    }
   ],
   "source": [
    "# Run parallel parameter extraction\n",
    "param_results = extract_params_parallel(datasets, methods_names)\n",
    "\n",
    "np.save('2026-01-19-Mcomp-param_results.npy', param_results)\n",
    "\n",
    "# Save results\n",
    "joblib.dump(param_results, '2026-01-19-Mcomp-params.joblib')\n",
    "print(\"Parameter results saved to 2026-01-19-Mcomp-params.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qpb72pctj8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL TYPE DISTRIBUTION BY METHOD\n",
      "======================================================================\n",
      "\n",
      "ADAM ETS Back:\n",
      "MAN     754\n",
      "MAA     434\n",
      "AAN     421\n",
      "MAM     375\n",
      "MNM     370\n",
      "MNN     243\n",
      "ANN     226\n",
      "ANA     219\n",
      "AAA     188\n",
      "MAdN    168\n",
      "\n",
      "ADAM ETS Opt:\n",
      "MAN     775\n",
      "MNN     605\n",
      "ANN     490\n",
      "AAN     392\n",
      "MAM     331\n",
      "MAdN    237\n",
      "MNA     226\n",
      "AAdN    213\n",
      "MNM     169\n",
      "ANM     148\n",
      "\n",
      "ADAM ETS Two:\n",
      "MAN     775\n",
      "MNN     605\n",
      "ANN     490\n",
      "AAN     392\n",
      "MAM     331\n",
      "MAdN    237\n",
      "MNA     226\n",
      "AAdN    213\n",
      "MNM     169\n",
      "ANM     148\n",
      "\n",
      "ES Back:\n",
      "MAN     754\n",
      "MAA     435\n",
      "AAN     417\n",
      "MAM     388\n",
      "MNM     375\n",
      "ANN     239\n",
      "MNN     229\n",
      "ANA     225\n",
      "AAA     190\n",
      "MAdN    166\n",
      "\n",
      "ES Opt:\n",
      "MAN     771\n",
      "MNN     621\n",
      "ANN     475\n",
      "AAN     380\n",
      "MAM     312\n",
      "MNA     232\n",
      "MAdN    227\n",
      "AAdN    225\n",
      "MNM     209\n",
      "ANA     126\n",
      "\n",
      "ES Two:\n",
      "MAN     771\n",
      "MNN     621\n",
      "ANN     475\n",
      "AAN     380\n",
      "MAM     312\n",
      "MNA     232\n",
      "MAdN    227\n",
      "AAdN    225\n",
      "MNM     209\n",
      "ANA     126\n"
     ]
    }
   ],
   "source": [
    "# param_results = np.load('2026-01-19-Mcomp-param_results.npy')\n",
    "\n",
    "# Analyze model type distributions\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL TYPE DISTRIBUTION BY METHOD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for method in methods_names:\n",
    "    model_types = param_results[method]['model_types']\n",
    "    # Count occurrences of each model type\n",
    "    type_counts = pd.Series(model_types).value_counts()\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(type_counts.head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dewllxjgw84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PARAMETER DISTRIBUTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ADAM ETS Back:\n",
      "------------------------------------------------------------\n",
      "    Parameter  Count     Mean      Std       Min      Q25   Median      Q75       Max  Where\n",
      "Phi (damping)     69   0.8913   0.1611    0.2581   0.8554   0.9470   1.0000    1.0000      0\n",
      "   Loss value   4004 399.4577 279.3122 -349.9066 169.2847 321.5593 600.6941 1401.8902    474\n",
      "\n",
      "ADAM ETS Opt:\n",
      "------------------------------------------------------------\n",
      "    Parameter  Count     Mean      Std       Min      Q25   Median      Q75       Max  Where\n",
      "Phi (damping)     70   0.8420   0.1383    0.4607   0.8180   0.8875   0.9447    0.9976      0\n",
      "   Loss value   4004 399.7592 278.6689 -349.3583 169.2995 320.7640 601.3232 1398.6604    474\n",
      "\n",
      "ADAM ETS Two:\n",
      "------------------------------------------------------------\n",
      "    Parameter  Count                                                                                                                                                                                                                                                                                                            Mean    Std       Min      Q25   Median      Q75                                                                                                                                                                                                                                                                                                                Max  Where\n",
      "Phi (damping)     70                                                                                                                                                                                                                                                                                                          0.8567 0.1200    0.5383   0.8212   0.8950   0.9457                                                                                                                                                                                                                                                                                                             1.0000      0\n",
      "   Loss value   4004 5744255744255744043211901397545139400383368254050717440397428317818994075527153037607690901702171889713490716168801536043224222618604604524666276460755599745899555421499421432984475058148816953536210616895204525224645124051819395127791557692277230352856919729692251177691487895799210185440911949824.0000    inf -349.1008 170.7723 323.6743 607.6329 1000000000000000052504760255204420248704468581108159154915854115511802457988908195786371375080447864043704443832883878176942523235360430575644792184786706982848387200926575803737830233794788090059368953234970799945081119038967640880074652742780142494579258788820056842838115669472196386865459400540160.0000    203\n",
      "\n",
      "ES Back:\n",
      "------------------------------------------------------------\n",
      "    Parameter  Count     Mean      Std       Min      Q25   Median      Q75       Max  Where\n",
      "Phi (damping)     69   0.9117   0.1485    0.2581   0.8950   0.9924   1.0000    1.0000      0\n",
      "   Loss value   4004 399.6591 279.7043 -349.8995 169.2656 321.1081 600.7808 1403.6539    474\n",
      "\n",
      "ES Opt:\n",
      "------------------------------------------------------------\n",
      "    Parameter  Count     Mean      Std       Min      Q25   Median      Q75       Max  Where\n",
      "Phi (damping)     72   0.8454   0.1407    0.4141   0.8073   0.8895   0.9478    1.0000      0\n",
      "   Loss value   4004 399.8961 278.8322 -349.3583 170.0899 320.6184 601.2995 1399.4928    474\n",
      "\n",
      "ES Two:\n",
      "------------------------------------------------------------\n",
      "    Parameter  Count                                                                                                                                                                                                                                                                                                            Mean    Std       Min      Q25   Median      Q75                                                                                                                                                                                                                                                                                                                Max  Where\n",
      "Phi (damping)     72                                                                                                                                                                                                                                                                                                          0.8696 0.1229    0.5303   0.8324   0.9067   0.9563                                                                                                                                                                                                                                                                                                             1.0000      0\n",
      "   Loss value   4004 5494505494505494857813625564214837509968856307166248796380903521845174007484207948858009665637950727282043411176403116262335455199137541386380436349485070466836544592247438558611483103244881072728607308051768512328707890596638703585049118215733959798387880551065648991042406900179646735171023011840.0000    inf -349.1008 170.9447 323.2998 608.5151 1000000000000000052504760255204420248704468581108159154915854115511802457988908195786371375080447864043704443832883878176942523235360430575644792184786706982848387200926575803737830233794788090059368953234970799945081119038967640880074652742780142494579258788820056842838115669472196386865459400540160.0000    349\n"
     ]
    }
   ],
   "source": [
    "# Parameter distribution summary\n",
    "print(\"=\" * 70)\n",
    "print(\"PARAMETER DISTRIBUTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "param_names = ['alpha', 'beta', 'gamma', 'phi', 'loss_values']\n",
    "param_labels = ['Alpha (level)', 'Beta (trend)', 'Gamma (seasonal)', 'Phi (damping)', 'Loss value']\n",
    "\n",
    "for method in methods_names:\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    summary_data = []\n",
    "    for param, label in zip(param_names, param_labels):\n",
    "        values = param_results[method][param]\n",
    "        valid = values[~np.isnan(values)]\n",
    "        if len(valid) > 0:\n",
    "            summary_data.append({\n",
    "                'Parameter': label,\n",
    "                'Count': len(valid),\n",
    "                'Mean': np.mean(valid),\n",
    "                'Std': np.std(valid),\n",
    "                'Min': np.min(valid),\n",
    "                'Q25': np.percentile(valid, 25),\n",
    "                'Median': np.median(valid),\n",
    "                'Q75': np.percentile(valid, 75),\n",
    "                'Max': np.max(valid),\n",
    "                'Where': np.argmax(values)\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        print(df.to_string(index=False, float_format=lambda x: f'{x:.4f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421g1uahfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Easy access to individual values from the dictionary\n",
    "print(\"=\" * 70)\n",
    "print(\"EXAMPLE: ACCESSING INDIVIDUAL VALUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get all alpha values for ADAM ETS Back method\n",
    "method = \"ADAM ETS Back\"\n",
    "print(f\"\\n1. All alpha values for {method}:\")\n",
    "print(f\"   param_results['{method}']['alpha'][:10] = {param_results[method]['alpha'][:10]}\")\n",
    "\n",
    "# Get model types for specific series indices\n",
    "print(f\"\\n2. Model types for first 5 series ({method}):\")\n",
    "for i in range(5):\n",
    "    print(f\"   Series {i}: {param_results[method]['model_types'][i]}\")\n",
    "\n",
    "# Filter by model type\n",
    "print(f\"\\n3. Alpha values for models with trend (containing 'A' or 'M' in position 2):\")\n",
    "model_types = param_results[method]['model_types']\n",
    "alpha_vals = param_results[method]['alpha']\n",
    "trend_mask = [m is not None and len(m) >= 2 and m[1] in ['A', 'M'] for m in model_types]\n",
    "alpha_with_trend = alpha_vals[trend_mask]\n",
    "print(f\"   Count: {len(alpha_with_trend)}, Mean: {np.nanmean(alpha_with_trend):.4f}\")\n",
    "\n",
    "# Compare parameters across methods\n",
    "print(f\"\\n4. Mean alpha across all methods:\")\n",
    "for method in methods_names:\n",
    "    mean_alpha = np.nanmean(param_results[method]['alpha'])\n",
    "    print(f\"   {method}: {mean_alpha:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gkdgofs32aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary structure reference\n",
    "print(\"=\" * 70)\n",
    "print(\"PARAM_RESULTS DICTIONARY STRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "param_results = {\n",
    "    'ADAM ETS Back': {\n",
    "        'model_types': [...],      # list[str]: Model types (e.g., 'AAN', 'MAdM', 'ANN')\n",
    "        'loss_values': np.array,   # float: Loss function values (CF_value)\n",
    "        'alpha': np.array,         # float: Level smoothing parameter\n",
    "        'beta': np.array,          # float: Trend smoothing parameter (NaN if no trend)\n",
    "        'gamma': np.array,         # float: Seasonal smoothing parameter (NaN if no season)\n",
    "        'phi': np.array,           # float: Dampening parameter (1.0 if not damped)\n",
    "        'errors': [...],           # list[str|None]: Error messages (None if success)\n",
    "    },\n",
    "    'ADAM ETS Opt': { ... },\n",
    "    'ADAM ETS Two': { ... },\n",
    "    'ES Back': { ... },\n",
    "    'ES Opt': { ... },\n",
    "    'ES Two': { ... },\n",
    "}\n",
    "\n",
    "Each array has length = number of datasets (4004 for M1+M3).\n",
    "Access patterns:\n",
    "  - Single value:  param_results['ADAM ETS Back']['alpha'][0]\n",
    "  - All values:    param_results['ADAM ETS Back']['alpha']\n",
    "  - By condition:  param_results['ADAM ETS Back']['alpha'][mask]\n",
    "\"\"\")\n",
    "\n",
    "# Show actual structure\n",
    "print(\"\\nActual keys in param_results:\")\n",
    "print(f\"  Methods: {list(param_results.keys())}\")\n",
    "print(f\"  Per-method keys: {list(param_results[methods_names[0]].keys())}\")\n",
    "print(f\"  Array lengths: {len(param_results[methods_names[0]]['alpha'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (smooth)",
   "language": "python",
   "name": "smooth"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
